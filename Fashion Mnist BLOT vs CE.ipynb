{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3213f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./fmnist/checkpoint_epoch_0.pth\n",
      "Epoch 1, Loss: 1.6358853052419895\n",
      "Accuracy on test set: 88.26%\n",
      "Model saved to ./fmnist/checkpoint_epoch_1.pth\n",
      "Epoch 2, Loss: 1.5671520116232605\n",
      "Accuracy on test set: 89.65%\n",
      "Model saved to ./fmnist/checkpoint_epoch_2.pth\n",
      "Epoch 3, Loss: 1.555843675568668\n",
      "Accuracy on test set: 89.73%\n",
      "Model saved to ./fmnist/checkpoint_epoch_3.pth\n",
      "Epoch 4, Loss: 1.5483821566933509\n",
      "Accuracy on test set: 89.89%\n",
      "Model saved to ./fmnist/checkpoint_epoch_4.pth\n",
      "Epoch 5, Loss: 1.5436316428662362\n",
      "Accuracy on test set: 89.42%\n",
      "Model saved to ./fmnist/checkpoint_epoch_5.pth\n",
      "Epoch 6, Loss: 1.5390878092251352\n",
      "Accuracy on test set: 90.03%\n",
      "Model saved to ./fmnist/checkpoint_epoch_6.pth\n",
      "Epoch 7, Loss: 1.5369168391614072\n",
      "Accuracy on test set: 90.46%\n",
      "Model saved to ./fmnist/checkpoint_epoch_7.pth\n",
      "Epoch 8, Loss: 1.532227820679069\n",
      "Accuracy on test set: 90.2%\n",
      "Model saved to ./fmnist/checkpoint_epoch_8.pth\n",
      "Epoch 9, Loss: 1.5293533922766827\n",
      "Accuracy on test set: 90.95%\n",
      "Model saved to ./fmnist/checkpoint_epoch_9.pth\n",
      "Epoch 10, Loss: 1.527730601682846\n",
      "Accuracy on test set: 91.03%\n",
      "Model saved to ./fmnist/checkpoint_epoch_10.pth\n",
      "Epoch 11, Loss: 1.524949120178914\n",
      "Accuracy on test set: 91.31%\n",
      "Model saved to ./fmnist/checkpoint_epoch_11.pth\n",
      "Epoch 12, Loss: 1.5229203474801232\n",
      "Accuracy on test set: 91.17%\n",
      "Model saved to ./fmnist/checkpoint_epoch_12.pth\n",
      "Epoch 13, Loss: 1.5194734308257032\n",
      "Accuracy on test set: 91.16%\n",
      "Model saved to ./fmnist/checkpoint_epoch_13.pth\n",
      "Epoch 14, Loss: 1.5182201187493705\n",
      "Accuracy on test set: 91.38%\n",
      "Model saved to ./fmnist/checkpoint_epoch_14.pth\n",
      "Epoch 15, Loss: 1.5157705537800088\n",
      "Accuracy on test set: 91.01%\n",
      "Model saved to ./fmnist/checkpoint_epoch_15.pth\n",
      "Epoch 16, Loss: 1.5159858570678402\n",
      "Accuracy on test set: 91.14%\n",
      "Model saved to ./fmnist/checkpoint_epoch_16.pth\n",
      "Epoch 17, Loss: 1.5131485601986394\n",
      "Accuracy on test set: 91.24%\n",
      "Model saved to ./fmnist/checkpoint_epoch_17.pth\n",
      "Epoch 18, Loss: 1.5127675942520598\n",
      "Accuracy on test set: 91.4%\n",
      "Model saved to ./fmnist/checkpoint_epoch_18.pth\n",
      "Epoch 19, Loss: 1.5114305452751453\n",
      "Accuracy on test set: 91.54%\n",
      "Model saved to ./fmnist/checkpoint_epoch_19.pth\n",
      "Epoch 20, Loss: 1.5096738072854878\n",
      "Accuracy on test set: 91.89%\n",
      "Model saved to ./fmnist/checkpoint_epoch_20.pth\n",
      "Epoch 21, Loss: 1.5094634990956484\n",
      "Accuracy on test set: 91.5%\n",
      "Model saved to ./fmnist/checkpoint_epoch_21.pth\n",
      "Epoch 22, Loss: 1.5076794442591637\n",
      "Accuracy on test set: 91.56%\n",
      "Model saved to ./fmnist/checkpoint_epoch_22.pth\n",
      "Epoch 23, Loss: 1.5068293507419415\n",
      "Accuracy on test set: 91.86%\n",
      "Model saved to ./fmnist/checkpoint_epoch_23.pth\n",
      "Epoch 24, Loss: 1.505413894714323\n",
      "Accuracy on test set: 91.69%\n",
      "Model saved to ./fmnist/checkpoint_epoch_24.pth\n",
      "Epoch 25, Loss: 1.5040339059921215\n",
      "Accuracy on test set: 91.59%\n",
      "Model saved to ./fmnist/checkpoint_epoch_25.pth\n",
      "Epoch 26, Loss: 1.5029862393448348\n",
      "Accuracy on test set: 91.64%\n",
      "Model saved to ./fmnist/checkpoint_epoch_26.pth\n",
      "Epoch 27, Loss: 1.5024412355697485\n",
      "Accuracy on test set: 91.86%\n",
      "Model saved to ./fmnist/checkpoint_epoch_27.pth\n",
      "Epoch 28, Loss: 1.501920980176946\n",
      "Accuracy on test set: 92.13%\n",
      "Model saved to ./fmnist/checkpoint_epoch_28.pth\n",
      "Epoch 29, Loss: 1.5002989532596775\n",
      "Accuracy on test set: 91.92%\n",
      "Model saved to ./fmnist/checkpoint_epoch_29.pth\n",
      "Epoch 30, Loss: 1.4994763393900288\n",
      "Accuracy on test set: 91.87%\n",
      "Model saved to ./fmnist/checkpoint_epoch_30.pth\n",
      "Epoch 31, Loss: 1.4991011458165102\n",
      "Accuracy on test set: 91.33%\n",
      "Model saved to ./fmnist/checkpoint_epoch_31.pth\n",
      "Epoch 32, Loss: 1.4992045078958784\n",
      "Accuracy on test set: 91.56%\n",
      "Model saved to ./fmnist/checkpoint_epoch_32.pth\n",
      "Epoch 33, Loss: 1.4993573682648795\n",
      "Accuracy on test set: 91.99%\n",
      "Model saved to ./fmnist/checkpoint_epoch_33.pth\n",
      "Epoch 34, Loss: 1.4963572944151058\n",
      "Accuracy on test set: 91.58%\n",
      "Model saved to ./fmnist/checkpoint_epoch_34.pth\n",
      "Epoch 35, Loss: 1.4971550226465726\n",
      "Accuracy on test set: 91.74%\n",
      "Model saved to ./fmnist/checkpoint_epoch_35.pth\n",
      "Epoch 36, Loss: 1.4961774485197656\n",
      "Accuracy on test set: 91.57%\n",
      "Model saved to ./fmnist/checkpoint_epoch_36.pth\n",
      "Epoch 37, Loss: 1.4966595455019205\n",
      "Accuracy on test set: 91.48%\n",
      "Model saved to ./fmnist/checkpoint_epoch_37.pth\n",
      "Epoch 38, Loss: 1.494352827702504\n",
      "Accuracy on test set: 91.94%\n",
      "Model saved to ./fmnist/checkpoint_epoch_38.pth\n",
      "Epoch 39, Loss: 1.4947063840274364\n",
      "Accuracy on test set: 92.16%\n",
      "Model saved to ./fmnist/checkpoint_epoch_39.pth\n",
      "Epoch 40, Loss: 1.4937801471651235\n",
      "Accuracy on test set: 92.15%\n",
      "Model saved to ./fmnist/checkpoint_epoch_40.pth\n",
      "Epoch 41, Loss: 1.4926532207013192\n",
      "Accuracy on test set: 92.03%\n",
      "Model saved to ./fmnist/checkpoint_epoch_41.pth\n",
      "Epoch 42, Loss: 1.492974946112521\n",
      "Accuracy on test set: 91.7%\n",
      "Model saved to ./fmnist/checkpoint_epoch_42.pth\n",
      "Epoch 43, Loss: 1.4916723380719166\n",
      "Accuracy on test set: 91.96%\n",
      "Model saved to ./fmnist/checkpoint_epoch_43.pth\n",
      "Epoch 44, Loss: 1.4912180575226415\n",
      "Accuracy on test set: 91.83%\n",
      "Model saved to ./fmnist/checkpoint_epoch_44.pth\n",
      "Epoch 45, Loss: 1.4913707323420022\n",
      "Accuracy on test set: 91.8%\n",
      "Model saved to ./fmnist/checkpoint_epoch_45.pth\n",
      "Epoch 46, Loss: 1.4914986704712483\n",
      "Accuracy on test set: 91.59%\n",
      "Model saved to ./fmnist/checkpoint_epoch_46.pth\n",
      "Epoch 47, Loss: 1.491239356969211\n",
      "Accuracy on test set: 91.82%\n",
      "Model saved to ./fmnist/checkpoint_epoch_47.pth\n",
      "Epoch 48, Loss: 1.4899891569161974\n",
      "Accuracy on test set: 91.86%\n",
      "Model saved to ./fmnist/checkpoint_epoch_48.pth\n",
      "Epoch 49, Loss: 1.4895166326433356\n",
      "Accuracy on test set: 91.86%\n",
      "Model saved to ./fmnist/checkpoint_epoch_49.pth\n",
      "Epoch 50, Loss: 1.4889651742825376\n",
      "Accuracy on test set: 91.81%\n",
      "Model saved to ./fmnist/checkpoint_epoch_50.pth\n",
      "Epoch 51, Loss: 1.489935111770752\n",
      "Accuracy on test set: 92.0%\n",
      "Model saved to ./fmnist/checkpoint_epoch_51.pth\n",
      "Epoch 52, Loss: 1.488363127083158\n",
      "Accuracy on test set: 92.01%\n",
      "Model saved to ./fmnist/checkpoint_epoch_52.pth\n",
      "Epoch 53, Loss: 1.4884536698428807\n",
      "Accuracy on test set: 91.46%\n",
      "Model saved to ./fmnist/checkpoint_epoch_53.pth\n",
      "Epoch 54, Loss: 1.4881706209833434\n",
      "Accuracy on test set: 91.89%\n",
      "Model saved to ./fmnist/checkpoint_epoch_54.pth\n",
      "Epoch 55, Loss: 1.488020873400194\n",
      "Accuracy on test set: 91.99%\n",
      "Model saved to ./fmnist/checkpoint_epoch_55.pth\n",
      "Epoch 56, Loss: 1.4865477022840017\n",
      "Accuracy on test set: 91.81%\n",
      "Model saved to ./fmnist/checkpoint_epoch_56.pth\n",
      "Epoch 57, Loss: 1.4872137341163814\n",
      "Accuracy on test set: 91.93%\n",
      "Model saved to ./fmnist/checkpoint_epoch_57.pth\n",
      "Epoch 58, Loss: 1.4866250534809984\n",
      "Accuracy on test set: 91.78%\n",
      "Model saved to ./fmnist/checkpoint_epoch_58.pth\n",
      "Epoch 59, Loss: 1.4870436186475287\n",
      "Accuracy on test set: 91.53%\n",
      "Model saved to ./fmnist/checkpoint_epoch_59.pth\n",
      "Epoch 60, Loss: 1.486153539818233\n",
      "Accuracy on test set: 91.9%\n",
      "Model saved to ./fmnist/checkpoint_epoch_60.pth\n",
      "Epoch 61, Loss: 1.4863409295773455\n",
      "Accuracy on test set: 91.67%\n",
      "Model saved to ./fmnist/checkpoint_epoch_61.pth\n",
      "Epoch 62, Loss: 1.4856500438789824\n",
      "Accuracy on test set: 91.8%\n",
      "Model saved to ./fmnist/checkpoint_epoch_62.pth\n",
      "Epoch 63, Loss: 1.484768440474325\n",
      "Accuracy on test set: 91.53%\n",
      "Model saved to ./fmnist/checkpoint_epoch_63.pth\n",
      "Epoch 64, Loss: 1.4844815284966915\n",
      "Accuracy on test set: 92.05%\n",
      "Model saved to ./fmnist/checkpoint_epoch_64.pth\n",
      "Epoch 65, Loss: 1.4842632770029975\n",
      "Accuracy on test set: 91.88%\n",
      "Model saved to ./fmnist/checkpoint_epoch_65.pth\n",
      "Epoch 66, Loss: 1.4846900072433293\n",
      "Accuracy on test set: 91.81%\n",
      "Model saved to ./fmnist/checkpoint_epoch_66.pth\n",
      "Epoch 67, Loss: 1.4845869436955401\n",
      "Accuracy on test set: 91.47%\n",
      "Model saved to ./fmnist/checkpoint_epoch_67.pth\n",
      "Epoch 68, Loss: 1.4843961126260412\n",
      "Accuracy on test set: 91.94%\n",
      "Model saved to ./fmnist/checkpoint_epoch_68.pth\n",
      "Epoch 69, Loss: 1.4836347363650926\n",
      "Accuracy on test set: 91.89%\n",
      "Model saved to ./fmnist/checkpoint_epoch_69.pth\n",
      "Epoch 70, Loss: 1.483584298762177\n",
      "Accuracy on test set: 91.97%\n",
      "Model saved to ./fmnist/checkpoint_epoch_70.pth\n",
      "Epoch 71, Loss: 1.4839957603005205\n",
      "Accuracy on test set: 91.93%\n",
      "Model saved to ./fmnist/checkpoint_epoch_71.pth\n",
      "Epoch 72, Loss: 1.4834586979229567\n",
      "Accuracy on test set: 92.16%\n",
      "Model saved to ./fmnist/checkpoint_epoch_72.pth\n",
      "Epoch 73, Loss: 1.4834368692786455\n",
      "Accuracy on test set: 91.93%\n",
      "Model saved to ./fmnist/checkpoint_epoch_73.pth\n",
      "Epoch 74, Loss: 1.4822298673424386\n",
      "Accuracy on test set: 91.9%\n",
      "Model saved to ./fmnist/checkpoint_epoch_74.pth\n",
      "Epoch 75, Loss: 1.4828882554192533\n",
      "Accuracy on test set: 92.07%\n",
      "Model saved to ./fmnist/checkpoint_epoch_75.pth\n",
      "Epoch 76, Loss: 1.4824631146784784\n",
      "Accuracy on test set: 91.87%\n",
      "Model saved to ./fmnist/checkpoint_epoch_76.pth\n",
      "Epoch 77, Loss: 1.482872972356231\n",
      "Accuracy on test set: 91.7%\n",
      "Model saved to ./fmnist/checkpoint_epoch_77.pth\n",
      "Epoch 78, Loss: 1.482519957810831\n",
      "Accuracy on test set: 91.68%\n",
      "Model saved to ./fmnist/checkpoint_epoch_78.pth\n",
      "Epoch 79, Loss: 1.4823605252989829\n",
      "Accuracy on test set: 92.28%\n",
      "Model saved to ./fmnist/checkpoint_epoch_79.pth\n",
      "Epoch 80, Loss: 1.4826991963488207\n",
      "Accuracy on test set: 91.74%\n",
      "Model saved to ./fmnist/checkpoint_epoch_80.pth\n",
      "Epoch 81, Loss: 1.4822423976621648\n",
      "Accuracy on test set: 92.06%\n",
      "Model saved to ./fmnist/checkpoint_epoch_81.pth\n",
      "Epoch 82, Loss: 1.4822009456183103\n",
      "Accuracy on test set: 92.24%\n",
      "Model saved to ./fmnist/checkpoint_epoch_82.pth\n",
      "Epoch 83, Loss: 1.4816882579819735\n",
      "Accuracy on test set: 91.89%\n",
      "Model saved to ./fmnist/checkpoint_epoch_83.pth\n",
      "Epoch 84, Loss: 1.4811786504696682\n",
      "Accuracy on test set: 91.94%\n",
      "Model saved to ./fmnist/checkpoint_epoch_84.pth\n",
      "Epoch 85, Loss: 1.4813604493385184\n",
      "Accuracy on test set: 91.92%\n",
      "Model saved to ./fmnist/checkpoint_epoch_85.pth\n",
      "Epoch 86, Loss: 1.481174971121969\n",
      "Accuracy on test set: 91.96%\n",
      "Model saved to ./fmnist/checkpoint_epoch_86.pth\n",
      "Epoch 87, Loss: 1.4812323221011456\n",
      "Accuracy on test set: 92.33%\n",
      "Model saved to ./fmnist/checkpoint_epoch_87.pth\n",
      "Epoch 88, Loss: 1.4812928121735547\n",
      "Accuracy on test set: 92.49%\n",
      "Model saved to ./fmnist/checkpoint_epoch_88.pth\n",
      "Epoch 89, Loss: 1.4807228623931088\n",
      "Accuracy on test set: 92.27%\n",
      "Model saved to ./fmnist/checkpoint_epoch_89.pth\n",
      "Epoch 90, Loss: 1.4801167716095442\n",
      "Accuracy on test set: 92.21%\n",
      "Model saved to ./fmnist/checkpoint_epoch_90.pth\n",
      "Epoch 91, Loss: 1.479912124335893\n",
      "Accuracy on test set: 91.9%\n",
      "Model saved to ./fmnist/checkpoint_epoch_91.pth\n",
      "Epoch 92, Loss: 1.4801700199082461\n",
      "Accuracy on test set: 91.94%\n",
      "Model saved to ./fmnist/checkpoint_epoch_92.pth\n",
      "Epoch 93, Loss: 1.479861553162654\n",
      "Accuracy on test set: 91.44%\n",
      "Model saved to ./fmnist/checkpoint_epoch_93.pth\n",
      "Epoch 94, Loss: 1.4790974904987604\n",
      "Accuracy on test set: 91.6%\n",
      "Model saved to ./fmnist/checkpoint_epoch_94.pth\n",
      "Epoch 95, Loss: 1.479928844519007\n",
      "Accuracy on test set: 91.97%\n",
      "Model saved to ./fmnist/checkpoint_epoch_95.pth\n",
      "Epoch 96, Loss: 1.4791942294726748\n",
      "Accuracy on test set: 91.64%\n",
      "Model saved to ./fmnist/checkpoint_epoch_96.pth\n",
      "Epoch 97, Loss: 1.4804082976729631\n",
      "Accuracy on test set: 91.76%\n",
      "Model saved to ./fmnist/checkpoint_epoch_97.pth\n",
      "Epoch 98, Loss: 1.4796576907893997\n",
      "Accuracy on test set: 91.79%\n",
      "Model saved to ./fmnist/checkpoint_epoch_98.pth\n",
      "Epoch 99, Loss: 1.4787918735923036\n",
      "Accuracy on test set: 91.81%\n",
      "Model saved to ./fmnist/checkpoint_epoch_99.pth\n",
      "Epoch 100, Loss: 1.4790135188651745\n",
      "Accuracy on test set: 91.66%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define CNN architecture\n",
    "\n",
    "# Define CNN architecture with Batch Normalization\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch Norm\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch Norm\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Batch Norm\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 128)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(128)  # Batch Norm\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(64)  # Batch Norm\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Apply Batch Norm\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Apply Batch Norm\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Apply Batch Norm\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))  # Apply Batch Norm\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn_fc2(self.fc2(x)))  # Apply Batch Norm\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Check for GPU and use it if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FashionMNISTModel().to(device)\n",
    "def save_checkpoint(epoch, model, optimizer, path='./fmnist/checkpoint.pth'):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    try:\n",
    "        torch.save(state, path)\n",
    "        print(f'Model saved to {path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error saving model: {e}')\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) \n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    save_checkpoint(epoch, model, optimizer, path=f'./fmnist/checkpoint_epoch_{epoch}.pth')\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
    "    # Testing the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d9680",
   "metadata": {},
   "source": [
    "# BLOT LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41c7dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/60000] Loss: 1.263420 | Grad Norm: 0.999999\n",
      "Epoch 1 [6400/60000] Loss: 0.470093 | Grad Norm: 0.316115\n",
      "Epoch 1 [12800/60000] Loss: 0.361460 | Grad Norm: 0.157285\n",
      "Epoch 1 [19200/60000] Loss: 0.317149 | Grad Norm: 0.303545\n",
      "Epoch 1 [25600/60000] Loss: 0.290204 | Grad Norm: 0.328989\n",
      "Epoch 1 [32000/60000] Loss: 0.273240 | Grad Norm: 0.344299\n",
      "Epoch 1 [38400/60000] Loss: 0.260978 | Grad Norm: 0.360410\n",
      "Epoch 1 [44800/60000] Loss: 0.251338 | Grad Norm: 0.489539\n",
      "Epoch 1 [51200/60000] Loss: 0.242995 | Grad Norm: 0.261258\n",
      "Epoch 1 [57600/60000] Loss: 0.237226 | Grad Norm: 0.517022\n",
      "Accuracy on test images: 83.45%\n",
      "Epoch 2 [0/60000] Loss: 0.202385 | Grad Norm: 0.230545\n",
      "Epoch 2 [6400/60000] Loss: 0.181345 | Grad Norm: 0.556004\n",
      "Epoch 2 [12800/60000] Loss: 0.181857 | Grad Norm: 0.111720\n",
      "Epoch 2 [19200/60000] Loss: 0.179320 | Grad Norm: 0.161669\n",
      "Epoch 2 [25600/60000] Loss: 0.181892 | Grad Norm: 0.422458\n",
      "Epoch 2 [32000/60000] Loss: 0.180496 | Grad Norm: 0.307625\n",
      "Epoch 2 [38400/60000] Loss: 0.181375 | Grad Norm: 0.196577\n",
      "Epoch 2 [44800/60000] Loss: 0.181311 | Grad Norm: 0.386255\n",
      "Epoch 2 [51200/60000] Loss: 0.179778 | Grad Norm: 0.314014\n",
      "Epoch 2 [57600/60000] Loss: 0.179819 | Grad Norm: 0.272176\n",
      "Accuracy on test images: 84.52%\n",
      "Epoch 3 [0/60000] Loss: 0.141485 | Grad Norm: 0.057644\n",
      "Epoch 3 [6400/60000] Loss: 0.173009 | Grad Norm: 0.131999\n",
      "Epoch 3 [12800/60000] Loss: 0.169339 | Grad Norm: 0.696847\n",
      "Epoch 3 [19200/60000] Loss: 0.168870 | Grad Norm: 0.266042\n",
      "Epoch 3 [25600/60000] Loss: 0.169666 | Grad Norm: 0.401807\n",
      "Epoch 3 [32000/60000] Loss: 0.171126 | Grad Norm: 0.212842\n",
      "Epoch 3 [38400/60000] Loss: 0.171564 | Grad Norm: 0.310950\n",
      "Epoch 3 [44800/60000] Loss: 0.171526 | Grad Norm: 0.081473\n",
      "Epoch 3 [51200/60000] Loss: 0.169616 | Grad Norm: 0.314054\n",
      "Epoch 3 [57600/60000] Loss: 0.169254 | Grad Norm: 0.338454\n",
      "Accuracy on test images: 84.13%\n",
      "Epoch 4 [0/60000] Loss: 0.219020 | Grad Norm: 0.366197\n",
      "Epoch 4 [6400/60000] Loss: 0.166629 | Grad Norm: 0.126157\n",
      "Epoch 4 [12800/60000] Loss: 0.166233 | Grad Norm: 0.651447\n",
      "Epoch 4 [19200/60000] Loss: 0.165243 | Grad Norm: 0.152569\n",
      "Epoch 4 [25600/60000] Loss: 0.164711 | Grad Norm: 0.337203\n",
      "Epoch 4 [32000/60000] Loss: 0.165236 | Grad Norm: 0.706140\n",
      "Epoch 4 [38400/60000] Loss: 0.165707 | Grad Norm: 0.945032\n",
      "Epoch 4 [44800/60000] Loss: 0.166054 | Grad Norm: 0.550653\n",
      "Epoch 4 [51200/60000] Loss: 0.165338 | Grad Norm: 0.011376\n",
      "Epoch 4 [57600/60000] Loss: 0.164517 | Grad Norm: 0.011933\n",
      "Accuracy on test images: 84.74%\n",
      "Epoch 5 [0/60000] Loss: 0.052765 | Grad Norm: 0.463075\n",
      "Epoch 5 [6400/60000] Loss: 0.160909 | Grad Norm: 0.047763\n",
      "Epoch 5 [12800/60000] Loss: 0.160716 | Grad Norm: 0.480127\n",
      "Epoch 5 [19200/60000] Loss: 0.160222 | Grad Norm: 0.597202\n",
      "Epoch 5 [25600/60000] Loss: 0.159813 | Grad Norm: 0.236769\n",
      "Epoch 5 [32000/60000] Loss: 0.159676 | Grad Norm: 0.654767\n",
      "Epoch 5 [38400/60000] Loss: 0.160329 | Grad Norm: 0.614407\n",
      "Epoch 5 [44800/60000] Loss: 0.160445 | Grad Norm: 0.836394\n",
      "Epoch 5 [51200/60000] Loss: 0.160459 | Grad Norm: 0.285342\n",
      "Epoch 5 [57600/60000] Loss: 0.159944 | Grad Norm: 0.153965\n",
      "Accuracy on test images: 85.02%\n",
      "Epoch 6 [0/60000] Loss: 0.170230 | Grad Norm: 0.065295\n",
      "Epoch 6 [6400/60000] Loss: 0.152015 | Grad Norm: 0.999999\n",
      "Epoch 6 [12800/60000] Loss: 0.155490 | Grad Norm: 0.999999\n",
      "Epoch 6 [19200/60000] Loss: 0.154207 | Grad Norm: 0.171490\n",
      "Epoch 6 [25600/60000] Loss: 0.155216 | Grad Norm: 0.484709\n",
      "Epoch 6 [32000/60000] Loss: 0.154637 | Grad Norm: 0.210731\n",
      "Epoch 6 [38400/60000] Loss: 0.155398 | Grad Norm: 0.401369\n",
      "Epoch 6 [44800/60000] Loss: 0.155189 | Grad Norm: 0.999999\n",
      "Epoch 6 [51200/60000] Loss: 0.154086 | Grad Norm: 0.957087\n",
      "Epoch 6 [57600/60000] Loss: 0.154592 | Grad Norm: 0.999999\n",
      "Accuracy on test images: 83.50%\n",
      "Epoch 7 [0/60000] Loss: 0.213610 | Grad Norm: 0.575925\n",
      "Epoch 7 [6400/60000] Loss: 0.154682 | Grad Norm: 0.474512\n",
      "Epoch 7 [12800/60000] Loss: 0.159667 | Grad Norm: 0.504033\n",
      "Epoch 7 [19200/60000] Loss: 0.158342 | Grad Norm: 0.843262\n",
      "Epoch 7 [25600/60000] Loss: 0.157798 | Grad Norm: 0.459903\n",
      "Epoch 7 [32000/60000] Loss: 0.157033 | Grad Norm: 0.890582\n",
      "Epoch 7 [38400/60000] Loss: 0.155167 | Grad Norm: 0.210248\n",
      "Epoch 7 [44800/60000] Loss: 0.153871 | Grad Norm: 0.780502\n",
      "Epoch 7 [51200/60000] Loss: 0.153549 | Grad Norm: 0.287933\n",
      "Epoch 7 [57600/60000] Loss: 0.154075 | Grad Norm: 0.543062\n",
      "Accuracy on test images: 85.24%\n",
      "Epoch 8 [0/60000] Loss: 0.165641 | Grad Norm: 0.999999\n",
      "Epoch 8 [6400/60000] Loss: 0.150871 | Grad Norm: 0.391760\n",
      "Epoch 8 [12800/60000] Loss: 0.148077 | Grad Norm: 0.341322\n",
      "Epoch 8 [19200/60000] Loss: 0.148537 | Grad Norm: 0.214072\n",
      "Epoch 8 [25600/60000] Loss: 0.148987 | Grad Norm: 0.674233\n",
      "Epoch 8 [32000/60000] Loss: 0.148709 | Grad Norm: 0.024518\n",
      "Epoch 8 [38400/60000] Loss: 0.150215 | Grad Norm: 0.442083\n",
      "Epoch 8 [44800/60000] Loss: 0.150876 | Grad Norm: 0.735161\n",
      "Epoch 8 [51200/60000] Loss: 0.150563 | Grad Norm: 0.343829\n",
      "Epoch 8 [57600/60000] Loss: 0.150952 | Grad Norm: 0.077818\n",
      "Accuracy on test images: 85.60%\n",
      "Epoch 9 [0/60000] Loss: 0.068593 | Grad Norm: 0.308058\n",
      "Epoch 9 [6400/60000] Loss: 0.140238 | Grad Norm: 0.135772\n",
      "Epoch 9 [12800/60000] Loss: 0.141753 | Grad Norm: 0.686632\n",
      "Epoch 9 [19200/60000] Loss: 0.141756 | Grad Norm: 1.000000\n",
      "Epoch 9 [25600/60000] Loss: 0.143662 | Grad Norm: 0.542166\n",
      "Epoch 9 [32000/60000] Loss: 0.144301 | Grad Norm: 0.243926\n",
      "Epoch 9 [38400/60000] Loss: 0.145868 | Grad Norm: 0.575370\n",
      "Epoch 9 [44800/60000] Loss: 0.146268 | Grad Norm: 0.843196\n",
      "Epoch 9 [51200/60000] Loss: 0.146941 | Grad Norm: 0.373630\n",
      "Epoch 9 [57600/60000] Loss: 0.147636 | Grad Norm: 0.632296\n",
      "Accuracy on test images: 85.17%\n",
      "Epoch 10 [0/60000] Loss: 0.184853 | Grad Norm: 0.999999\n",
      "Epoch 10 [6400/60000] Loss: 0.141976 | Grad Norm: 0.334508\n",
      "Epoch 10 [12800/60000] Loss: 0.140280 | Grad Norm: 0.001836\n",
      "Epoch 10 [19200/60000] Loss: 0.143940 | Grad Norm: 0.492177\n",
      "Epoch 10 [25600/60000] Loss: 0.146006 | Grad Norm: 0.000786\n",
      "Epoch 10 [32000/60000] Loss: 0.147281 | Grad Norm: 0.999999\n",
      "Epoch 10 [38400/60000] Loss: 0.147326 | Grad Norm: 0.213571\n",
      "Epoch 10 [44800/60000] Loss: 0.147555 | Grad Norm: 0.999999\n",
      "Epoch 10 [51200/60000] Loss: 0.147205 | Grad Norm: 0.180867\n",
      "Epoch 10 [57600/60000] Loss: 0.146676 | Grad Norm: 0.000748\n",
      "Accuracy on test images: 85.15%\n",
      "Epoch 11 [0/60000] Loss: 0.203154 | Grad Norm: 0.003779\n",
      "Epoch 11 [6400/60000] Loss: 0.136263 | Grad Norm: 0.578081\n",
      "Epoch 11 [12800/60000] Loss: 0.140584 | Grad Norm: 0.999999\n",
      "Epoch 11 [19200/60000] Loss: 0.141202 | Grad Norm: 0.250424\n",
      "Epoch 11 [25600/60000] Loss: 0.142635 | Grad Norm: 0.694648\n",
      "Epoch 11 [32000/60000] Loss: 0.142669 | Grad Norm: 0.061285\n",
      "Epoch 11 [38400/60000] Loss: 0.144147 | Grad Norm: 0.902525\n",
      "Epoch 11 [44800/60000] Loss: 0.145274 | Grad Norm: 0.377399\n",
      "Epoch 11 [51200/60000] Loss: 0.145149 | Grad Norm: 0.037614\n",
      "Epoch 11 [57600/60000] Loss: 0.144831 | Grad Norm: 0.024122\n",
      "Accuracy on test images: 85.46%\n",
      "Epoch 12 [0/60000] Loss: 0.215250 | Grad Norm: 0.262304\n",
      "Epoch 12 [6400/60000] Loss: 0.139532 | Grad Norm: 0.002254\n",
      "Epoch 12 [12800/60000] Loss: 0.140046 | Grad Norm: 0.517722\n",
      "Epoch 12 [19200/60000] Loss: 0.142098 | Grad Norm: 0.999999\n",
      "Epoch 12 [25600/60000] Loss: 0.141250 | Grad Norm: 0.453657\n",
      "Epoch 12 [32000/60000] Loss: 0.142039 | Grad Norm: 0.260129\n",
      "Epoch 12 [38400/60000] Loss: 0.142718 | Grad Norm: 0.820538\n",
      "Epoch 12 [44800/60000] Loss: 0.142097 | Grad Norm: 0.036637\n",
      "Epoch 12 [51200/60000] Loss: 0.143421 | Grad Norm: 0.496766\n",
      "Epoch 12 [57600/60000] Loss: 0.142897 | Grad Norm: 0.049538\n",
      "Accuracy on test images: 85.29%\n",
      "Epoch 13 [0/60000] Loss: 0.151838 | Grad Norm: 0.338473\n",
      "Epoch 13 [6400/60000] Loss: 0.146041 | Grad Norm: 0.002837\n",
      "Epoch 13 [12800/60000] Loss: 0.142325 | Grad Norm: 0.392645\n",
      "Epoch 13 [19200/60000] Loss: 0.142754 | Grad Norm: 0.119927\n",
      "Epoch 13 [25600/60000] Loss: 0.141739 | Grad Norm: 0.039904\n",
      "Epoch 13 [32000/60000] Loss: 0.142229 | Grad Norm: 0.209848\n",
      "Epoch 13 [38400/60000] Loss: 0.143456 | Grad Norm: 0.135269\n",
      "Epoch 13 [44800/60000] Loss: 0.142024 | Grad Norm: 0.966390\n",
      "Epoch 13 [51200/60000] Loss: 0.142917 | Grad Norm: 0.028637\n",
      "Epoch 13 [57600/60000] Loss: 0.142784 | Grad Norm: 0.033132\n",
      "Accuracy on test images: 85.58%\n",
      "Epoch 14 [0/60000] Loss: 0.123926 | Grad Norm: 0.080324\n",
      "Epoch 14 [6400/60000] Loss: 0.139418 | Grad Norm: 0.548521\n",
      "Epoch 14 [12800/60000] Loss: 0.138977 | Grad Norm: 0.003346\n",
      "Epoch 14 [19200/60000] Loss: 0.141684 | Grad Norm: 0.023158\n",
      "Epoch 14 [25600/60000] Loss: 0.141863 | Grad Norm: 0.676711\n",
      "Epoch 14 [32000/60000] Loss: 0.141720 | Grad Norm: 0.826343\n",
      "Epoch 14 [38400/60000] Loss: 0.141685 | Grad Norm: 0.999999\n",
      "Epoch 14 [44800/60000] Loss: 0.141118 | Grad Norm: 0.999999\n",
      "Epoch 14 [51200/60000] Loss: 0.141394 | Grad Norm: 0.552337\n",
      "Epoch 14 [57600/60000] Loss: 0.141246 | Grad Norm: 0.999999\n",
      "Accuracy on test images: 85.84%\n",
      "Epoch 15 [0/60000] Loss: 0.108231 | Grad Norm: 0.999999\n",
      "Epoch 15 [6400/60000] Loss: 0.140371 | Grad Norm: 0.238849\n",
      "Epoch 15 [12800/60000] Loss: 0.138099 | Grad Norm: 0.012978\n",
      "Epoch 15 [19200/60000] Loss: 0.140147 | Grad Norm: 0.971425\n",
      "Epoch 15 [25600/60000] Loss: 0.139420 | Grad Norm: 0.999999\n",
      "Epoch 15 [32000/60000] Loss: 0.139560 | Grad Norm: 0.728785\n",
      "Epoch 15 [38400/60000] Loss: 0.140288 | Grad Norm: 0.020625\n",
      "Epoch 15 [44800/60000] Loss: 0.140324 | Grad Norm: 0.360477\n",
      "Epoch 15 [51200/60000] Loss: 0.139567 | Grad Norm: 0.020208\n",
      "Epoch 15 [57600/60000] Loss: 0.139550 | Grad Norm: 0.662139\n",
      "Accuracy on test images: 85.46%\n",
      "Epoch 16 [0/60000] Loss: 0.119736 | Grad Norm: 0.601193\n",
      "Epoch 16 [6400/60000] Loss: 0.132016 | Grad Norm: 0.271576\n",
      "Epoch 16 [12800/60000] Loss: 0.136830 | Grad Norm: 0.759620\n",
      "Epoch 16 [19200/60000] Loss: 0.134517 | Grad Norm: 0.006117\n",
      "Epoch 16 [25600/60000] Loss: 0.135755 | Grad Norm: 0.994541\n",
      "Epoch 16 [32000/60000] Loss: 0.135459 | Grad Norm: 0.007691\n",
      "Epoch 16 [38400/60000] Loss: 0.135790 | Grad Norm: 0.666299\n",
      "Epoch 16 [44800/60000] Loss: 0.137103 | Grad Norm: 0.091515\n",
      "Epoch 16 [51200/60000] Loss: 0.136881 | Grad Norm: 0.000140\n",
      "Epoch 16 [57600/60000] Loss: 0.136655 | Grad Norm: 0.029412\n",
      "Accuracy on test images: 85.69%\n",
      "Epoch 17 [0/60000] Loss: 0.157092 | Grad Norm: 0.085283\n",
      "Epoch 17 [6400/60000] Loss: 0.131697 | Grad Norm: 0.219362\n",
      "Epoch 17 [12800/60000] Loss: 0.135681 | Grad Norm: 0.583286\n",
      "Epoch 17 [19200/60000] Loss: 0.137657 | Grad Norm: 0.048633\n",
      "Epoch 17 [25600/60000] Loss: 0.136748 | Grad Norm: 0.361382\n",
      "Epoch 17 [32000/60000] Loss: 0.137286 | Grad Norm: 0.422861\n",
      "Epoch 17 [38400/60000] Loss: 0.136300 | Grad Norm: 0.999999\n",
      "Epoch 17 [44800/60000] Loss: 0.137866 | Grad Norm: 0.835628\n",
      "Epoch 17 [51200/60000] Loss: 0.137668 | Grad Norm: 0.064454\n",
      "Epoch 17 [57600/60000] Loss: 0.137457 | Grad Norm: 0.026896\n",
      "Accuracy on test images: 85.96%\n",
      "Epoch 18 [0/60000] Loss: 0.125373 | Grad Norm: 0.029894\n",
      "Epoch 18 [6400/60000] Loss: 0.135436 | Grad Norm: 0.273470\n",
      "Epoch 18 [12800/60000] Loss: 0.132310 | Grad Norm: 0.032501\n",
      "Epoch 18 [19200/60000] Loss: 0.135220 | Grad Norm: 0.000417\n",
      "Epoch 18 [25600/60000] Loss: 0.134654 | Grad Norm: 0.108754\n",
      "Epoch 18 [32000/60000] Loss: 0.134654 | Grad Norm: 0.015729\n",
      "Epoch 18 [38400/60000] Loss: 0.135234 | Grad Norm: 0.039077\n",
      "Epoch 18 [44800/60000] Loss: 0.134980 | Grad Norm: 0.044951\n",
      "Epoch 18 [51200/60000] Loss: 0.134937 | Grad Norm: 0.338431\n",
      "Epoch 18 [57600/60000] Loss: 0.135361 | Grad Norm: 0.826101\n",
      "Accuracy on test images: 85.97%\n",
      "Epoch 19 [0/60000] Loss: 0.192210 | Grad Norm: 0.263811\n",
      "Epoch 19 [6400/60000] Loss: 0.133279 | Grad Norm: 0.878485\n",
      "Epoch 19 [12800/60000] Loss: 0.130617 | Grad Norm: 0.145572\n",
      "Epoch 19 [19200/60000] Loss: 0.133259 | Grad Norm: 0.115214\n",
      "Epoch 19 [25600/60000] Loss: 0.133400 | Grad Norm: 0.000335\n",
      "Epoch 19 [32000/60000] Loss: 0.132876 | Grad Norm: 0.000011\n",
      "Epoch 19 [38400/60000] Loss: 0.134416 | Grad Norm: 0.999999\n",
      "Epoch 19 [44800/60000] Loss: 0.133847 | Grad Norm: 0.112442\n",
      "Epoch 19 [51200/60000] Loss: 0.133781 | Grad Norm: 0.286336\n",
      "Epoch 19 [57600/60000] Loss: 0.134036 | Grad Norm: 0.002674\n",
      "Accuracy on test images: 85.47%\n",
      "Epoch 20 [0/60000] Loss: 0.078240 | Grad Norm: 0.012001\n",
      "Epoch 20 [6400/60000] Loss: 0.131080 | Grad Norm: 0.770836\n",
      "Epoch 20 [12800/60000] Loss: 0.131663 | Grad Norm: 0.010712\n",
      "Epoch 20 [19200/60000] Loss: 0.130633 | Grad Norm: 0.013473\n",
      "Epoch 20 [25600/60000] Loss: 0.128908 | Grad Norm: 0.030724\n",
      "Epoch 20 [32000/60000] Loss: 0.129969 | Grad Norm: 0.351385\n",
      "Epoch 20 [38400/60000] Loss: 0.130866 | Grad Norm: 0.090593\n",
      "Epoch 20 [44800/60000] Loss: 0.130643 | Grad Norm: 0.830553\n",
      "Epoch 20 [51200/60000] Loss: 0.132468 | Grad Norm: 0.000346\n",
      "Epoch 20 [57600/60000] Loss: 0.132665 | Grad Norm: 0.300457\n",
      "Accuracy on test images: 86.07%\n",
      "Epoch 21 [0/60000] Loss: 0.172520 | Grad Norm: 0.153795\n",
      "Epoch 21 [6400/60000] Loss: 0.133539 | Grad Norm: 0.000142\n",
      "Epoch 21 [12800/60000] Loss: 0.135293 | Grad Norm: 0.045228\n",
      "Epoch 21 [19200/60000] Loss: 0.133840 | Grad Norm: 0.542204\n",
      "Epoch 21 [25600/60000] Loss: 0.134163 | Grad Norm: 0.559215\n",
      "Epoch 21 [32000/60000] Loss: 0.134113 | Grad Norm: 0.653205\n",
      "Epoch 21 [38400/60000] Loss: 0.134028 | Grad Norm: 0.999999\n",
      "Epoch 21 [44800/60000] Loss: 0.133561 | Grad Norm: 0.999999\n",
      "Epoch 21 [51200/60000] Loss: 0.133544 | Grad Norm: 0.967530\n",
      "Epoch 21 [57600/60000] Loss: 0.133596 | Grad Norm: 0.143040\n",
      "Accuracy on test images: 86.11%\n",
      "Epoch 22 [0/60000] Loss: 0.180203 | Grad Norm: 0.999999\n",
      "Epoch 22 [6400/60000] Loss: 0.133825 | Grad Norm: 0.202321\n",
      "Epoch 22 [12800/60000] Loss: 0.134028 | Grad Norm: 0.002959\n",
      "Epoch 22 [19200/60000] Loss: 0.133348 | Grad Norm: 0.742849\n",
      "Epoch 22 [25600/60000] Loss: 0.134637 | Grad Norm: 0.359839\n",
      "Epoch 22 [32000/60000] Loss: 0.134829 | Grad Norm: 0.999999\n",
      "Epoch 22 [38400/60000] Loss: 0.133292 | Grad Norm: 0.272942\n",
      "Epoch 22 [44800/60000] Loss: 0.133189 | Grad Norm: 0.473656\n",
      "Epoch 22 [51200/60000] Loss: 0.132599 | Grad Norm: 0.002140\n",
      "Epoch 22 [57600/60000] Loss: 0.132671 | Grad Norm: 0.006357\n",
      "Accuracy on test images: 86.04%\n",
      "Epoch 23 [0/60000] Loss: 0.109456 | Grad Norm: 0.010569\n",
      "Epoch 23 [6400/60000] Loss: 0.132123 | Grad Norm: 0.007907\n",
      "Epoch 23 [12800/60000] Loss: 0.132820 | Grad Norm: 0.318790\n",
      "Epoch 23 [19200/60000] Loss: 0.132359 | Grad Norm: 0.001349\n",
      "Epoch 23 [25600/60000] Loss: 0.133087 | Grad Norm: 0.480782\n",
      "Epoch 23 [32000/60000] Loss: 0.132972 | Grad Norm: 0.838874\n",
      "Epoch 23 [38400/60000] Loss: 0.131579 | Grad Norm: 0.546788\n",
      "Epoch 23 [44800/60000] Loss: 0.131312 | Grad Norm: 0.847125\n",
      "Epoch 23 [51200/60000] Loss: 0.131735 | Grad Norm: 0.000016\n",
      "Epoch 23 [57600/60000] Loss: 0.132050 | Grad Norm: 0.002708\n",
      "Accuracy on test images: 85.74%\n",
      "Epoch 24 [0/60000] Loss: 0.168257 | Grad Norm: 0.498262\n",
      "Epoch 24 [6400/60000] Loss: 0.125991 | Grad Norm: 0.617783\n",
      "Epoch 24 [12800/60000] Loss: 0.127847 | Grad Norm: 0.001308\n",
      "Epoch 24 [19200/60000] Loss: 0.130047 | Grad Norm: 0.127588\n",
      "Epoch 24 [25600/60000] Loss: 0.131382 | Grad Norm: 0.113903\n",
      "Epoch 24 [32000/60000] Loss: 0.130016 | Grad Norm: 0.192966\n",
      "Epoch 24 [38400/60000] Loss: 0.130906 | Grad Norm: 0.184297\n",
      "Epoch 24 [44800/60000] Loss: 0.131430 | Grad Norm: 0.013750\n",
      "Epoch 24 [51200/60000] Loss: 0.131322 | Grad Norm: 0.740106\n",
      "Epoch 24 [57600/60000] Loss: 0.131294 | Grad Norm: 0.664239\n",
      "Accuracy on test images: 86.00%\n",
      "Epoch 25 [0/60000] Loss: 0.140662 | Grad Norm: 0.003286\n",
      "Epoch 25 [6400/60000] Loss: 0.125370 | Grad Norm: 0.235432\n",
      "Epoch 25 [12800/60000] Loss: 0.128635 | Grad Norm: 0.403257\n",
      "Epoch 25 [19200/60000] Loss: 0.130091 | Grad Norm: 0.094460\n",
      "Epoch 25 [25600/60000] Loss: 0.129788 | Grad Norm: 0.595784\n",
      "Epoch 25 [32000/60000] Loss: 0.128792 | Grad Norm: 0.629554\n",
      "Epoch 25 [38400/60000] Loss: 0.129419 | Grad Norm: 0.464278\n",
      "Epoch 25 [44800/60000] Loss: 0.129482 | Grad Norm: 0.340719\n",
      "Epoch 25 [51200/60000] Loss: 0.128807 | Grad Norm: 0.000904\n",
      "Epoch 25 [57600/60000] Loss: 0.129615 | Grad Norm: 0.000802\n",
      "Accuracy on test images: 86.00%\n",
      "Epoch 26 [0/60000] Loss: 0.078195 | Grad Norm: 0.004291\n",
      "Epoch 26 [6400/60000] Loss: 0.127043 | Grad Norm: 0.951578\n",
      "Epoch 26 [12800/60000] Loss: 0.129106 | Grad Norm: 0.049522\n",
      "Epoch 26 [19200/60000] Loss: 0.129813 | Grad Norm: 0.369429\n",
      "Epoch 26 [25600/60000] Loss: 0.130530 | Grad Norm: 0.005909\n",
      "Epoch 26 [32000/60000] Loss: 0.130878 | Grad Norm: 0.333074\n",
      "Epoch 26 [38400/60000] Loss: 0.130040 | Grad Norm: 0.554945\n",
      "Epoch 26 [44800/60000] Loss: 0.130639 | Grad Norm: 0.002832\n",
      "Epoch 26 [51200/60000] Loss: 0.130350 | Grad Norm: 0.189980\n",
      "Epoch 26 [57600/60000] Loss: 0.129501 | Grad Norm: 0.000010\n",
      "Accuracy on test images: 85.51%\n",
      "Epoch 27 [0/60000] Loss: 0.151695 | Grad Norm: 0.325398\n",
      "Epoch 27 [6400/60000] Loss: 0.132048 | Grad Norm: 0.402891\n",
      "Epoch 27 [12800/60000] Loss: 0.130200 | Grad Norm: 0.589772\n",
      "Epoch 27 [19200/60000] Loss: 0.131342 | Grad Norm: 0.074972\n",
      "Epoch 27 [25600/60000] Loss: 0.132530 | Grad Norm: 0.767629\n",
      "Epoch 27 [32000/60000] Loss: 0.131816 | Grad Norm: 0.442296\n",
      "Epoch 27 [38400/60000] Loss: 0.130212 | Grad Norm: 0.263466\n",
      "Epoch 27 [44800/60000] Loss: 0.129569 | Grad Norm: 0.002855\n",
      "Epoch 27 [51200/60000] Loss: 0.128866 | Grad Norm: 0.000348\n",
      "Epoch 27 [57600/60000] Loss: 0.128931 | Grad Norm: 0.060638\n",
      "Accuracy on test images: 85.48%\n",
      "Epoch 28 [0/60000] Loss: 0.098600 | Grad Norm: 0.478837\n",
      "Epoch 28 [6400/60000] Loss: 0.127058 | Grad Norm: 0.004745\n",
      "Epoch 28 [12800/60000] Loss: 0.127389 | Grad Norm: 0.004760\n",
      "Epoch 28 [19200/60000] Loss: 0.127184 | Grad Norm: 0.790789\n",
      "Epoch 28 [25600/60000] Loss: 0.129096 | Grad Norm: 0.179727\n",
      "Epoch 28 [32000/60000] Loss: 0.128957 | Grad Norm: 0.899895\n",
      "Epoch 28 [38400/60000] Loss: 0.129608 | Grad Norm: 0.016844\n",
      "Epoch 28 [44800/60000] Loss: 0.130011 | Grad Norm: 0.117541\n",
      "Epoch 28 [51200/60000] Loss: 0.130119 | Grad Norm: 0.000173\n",
      "Epoch 28 [57600/60000] Loss: 0.129748 | Grad Norm: 0.501603\n",
      "Accuracy on test images: 85.80%\n",
      "Epoch 29 [0/60000] Loss: 0.125241 | Grad Norm: 0.023685\n",
      "Epoch 29 [6400/60000] Loss: 0.132403 | Grad Norm: 0.950042\n",
      "Epoch 29 [12800/60000] Loss: 0.130377 | Grad Norm: 0.001701\n",
      "Epoch 29 [19200/60000] Loss: 0.128663 | Grad Norm: 0.036444\n",
      "Epoch 29 [25600/60000] Loss: 0.129289 | Grad Norm: 0.001744\n",
      "Epoch 29 [32000/60000] Loss: 0.127740 | Grad Norm: 0.488975\n",
      "Epoch 29 [38400/60000] Loss: 0.127836 | Grad Norm: 0.000061\n",
      "Epoch 29 [44800/60000] Loss: 0.128308 | Grad Norm: 0.041652\n",
      "Epoch 29 [51200/60000] Loss: 0.127964 | Grad Norm: 0.999999\n",
      "Epoch 29 [57600/60000] Loss: 0.127909 | Grad Norm: 0.602672\n",
      "Accuracy on test images: 86.09%\n",
      "Epoch 30 [0/60000] Loss: 0.109375 | Grad Norm: 0.000026\n",
      "Epoch 30 [6400/60000] Loss: 0.122309 | Grad Norm: 0.079621\n",
      "Epoch 30 [12800/60000] Loss: 0.130117 | Grad Norm: 0.001688\n",
      "Epoch 30 [19200/60000] Loss: 0.128981 | Grad Norm: 0.044154\n",
      "Epoch 30 [25600/60000] Loss: 0.129607 | Grad Norm: 0.081182\n",
      "Epoch 30 [32000/60000] Loss: 0.129151 | Grad Norm: 0.000026\n",
      "Epoch 30 [38400/60000] Loss: 0.128426 | Grad Norm: 0.778245\n",
      "Epoch 30 [44800/60000] Loss: 0.128643 | Grad Norm: 0.472834\n",
      "Epoch 30 [51200/60000] Loss: 0.129053 | Grad Norm: 0.093725\n",
      "Epoch 30 [57600/60000] Loss: 0.128596 | Grad Norm: 0.409790\n",
      "Accuracy on test images: 85.75%\n",
      "Epoch 31 [0/60000] Loss: 0.062850 | Grad Norm: 0.067527\n",
      "Epoch 31 [6400/60000] Loss: 0.125919 | Grad Norm: 0.027682\n",
      "Epoch 31 [12800/60000] Loss: 0.130085 | Grad Norm: 0.241786\n",
      "Epoch 31 [19200/60000] Loss: 0.125514 | Grad Norm: 0.465678\n",
      "Epoch 31 [25600/60000] Loss: 0.124851 | Grad Norm: 0.006357\n",
      "Epoch 31 [32000/60000] Loss: 0.125819 | Grad Norm: 0.000004\n",
      "Epoch 31 [38400/60000] Loss: 0.126059 | Grad Norm: 0.327283\n",
      "Epoch 31 [44800/60000] Loss: 0.125884 | Grad Norm: 0.159522\n",
      "Epoch 31 [51200/60000] Loss: 0.126590 | Grad Norm: 0.000012\n",
      "Epoch 31 [57600/60000] Loss: 0.125727 | Grad Norm: 0.007915\n",
      "Accuracy on test images: 86.14%\n",
      "Epoch 32 [0/60000] Loss: 0.094447 | Grad Norm: 0.048078\n",
      "Epoch 32 [6400/60000] Loss: 0.120149 | Grad Norm: 0.062226\n",
      "Epoch 32 [12800/60000] Loss: 0.122148 | Grad Norm: 0.522189\n",
      "Epoch 32 [19200/60000] Loss: 0.124007 | Grad Norm: 0.000403\n",
      "Epoch 32 [25600/60000] Loss: 0.125199 | Grad Norm: 0.353953\n",
      "Epoch 32 [32000/60000] Loss: 0.125540 | Grad Norm: 0.999999\n",
      "Epoch 32 [38400/60000] Loss: 0.125808 | Grad Norm: 0.011182\n",
      "Epoch 32 [44800/60000] Loss: 0.126118 | Grad Norm: 0.098030\n",
      "Epoch 32 [51200/60000] Loss: 0.126230 | Grad Norm: 0.396415\n",
      "Epoch 32 [57600/60000] Loss: 0.126457 | Grad Norm: 0.642894\n",
      "Accuracy on test images: 85.96%\n",
      "Epoch 33 [0/60000] Loss: 0.080222 | Grad Norm: 0.176721\n",
      "Epoch 33 [6400/60000] Loss: 0.131303 | Grad Norm: 0.000349\n",
      "Epoch 33 [12800/60000] Loss: 0.129568 | Grad Norm: 0.000660\n",
      "Epoch 33 [19200/60000] Loss: 0.129131 | Grad Norm: 0.891744\n",
      "Epoch 33 [25600/60000] Loss: 0.128653 | Grad Norm: 0.537860\n",
      "Epoch 33 [32000/60000] Loss: 0.127990 | Grad Norm: 0.999999\n",
      "Epoch 33 [38400/60000] Loss: 0.127308 | Grad Norm: 0.003560\n",
      "Epoch 33 [44800/60000] Loss: 0.126502 | Grad Norm: 0.000499\n",
      "Epoch 33 [51200/60000] Loss: 0.127104 | Grad Norm: 0.015922\n",
      "Epoch 33 [57600/60000] Loss: 0.126372 | Grad Norm: 0.000159\n",
      "Accuracy on test images: 86.09%\n",
      "Epoch 34 [0/60000] Loss: 0.125006 | Grad Norm: 0.000748\n",
      "Epoch 34 [6400/60000] Loss: 0.120388 | Grad Norm: 0.379728\n",
      "Epoch 34 [12800/60000] Loss: 0.124769 | Grad Norm: 0.087294\n",
      "Epoch 34 [19200/60000] Loss: 0.125537 | Grad Norm: 0.133334\n",
      "Epoch 34 [25600/60000] Loss: 0.126107 | Grad Norm: 0.000000\n",
      "Epoch 34 [32000/60000] Loss: 0.125764 | Grad Norm: 0.000177\n",
      "Epoch 34 [38400/60000] Loss: 0.125021 | Grad Norm: 0.000166\n",
      "Epoch 34 [44800/60000] Loss: 0.125544 | Grad Norm: 0.600757\n",
      "Epoch 34 [51200/60000] Loss: 0.125416 | Grad Norm: 0.000178\n",
      "Epoch 34 [57600/60000] Loss: 0.125464 | Grad Norm: 0.305025\n",
      "Accuracy on test images: 85.68%\n",
      "Epoch 35 [0/60000] Loss: 0.125400 | Grad Norm: 0.070703\n",
      "Epoch 35 [6400/60000] Loss: 0.128167 | Grad Norm: 0.300047\n",
      "Epoch 35 [12800/60000] Loss: 0.125620 | Grad Norm: 0.000070\n",
      "Epoch 35 [19200/60000] Loss: 0.123546 | Grad Norm: 0.000007\n",
      "Epoch 35 [25600/60000] Loss: 0.125784 | Grad Norm: 0.104105\n",
      "Epoch 35 [32000/60000] Loss: 0.126396 | Grad Norm: 0.002568\n",
      "Epoch 35 [38400/60000] Loss: 0.125478 | Grad Norm: 0.410809\n",
      "Epoch 35 [44800/60000] Loss: 0.125075 | Grad Norm: 0.121391\n",
      "Epoch 35 [51200/60000] Loss: 0.125381 | Grad Norm: 0.325822\n",
      "Epoch 35 [57600/60000] Loss: 0.124981 | Grad Norm: 0.342191\n",
      "Accuracy on test images: 85.97%\n",
      "Epoch 36 [0/60000] Loss: 0.109494 | Grad Norm: 0.040676\n",
      "Epoch 36 [6400/60000] Loss: 0.117763 | Grad Norm: 0.166365\n",
      "Epoch 36 [12800/60000] Loss: 0.120456 | Grad Norm: 0.055368\n",
      "Epoch 36 [19200/60000] Loss: 0.120927 | Grad Norm: 0.012010\n",
      "Epoch 36 [25600/60000] Loss: 0.123332 | Grad Norm: 0.088661\n",
      "Epoch 36 [32000/60000] Loss: 0.123276 | Grad Norm: 0.004427\n",
      "Epoch 36 [38400/60000] Loss: 0.123402 | Grad Norm: 0.164801\n",
      "Epoch 36 [44800/60000] Loss: 0.124295 | Grad Norm: 0.255147\n",
      "Epoch 36 [51200/60000] Loss: 0.123807 | Grad Norm: 0.005802\n",
      "Epoch 36 [57600/60000] Loss: 0.124086 | Grad Norm: 0.986536\n",
      "Accuracy on test images: 86.13%\n",
      "Epoch 37 [0/60000] Loss: 0.159542 | Grad Norm: 0.548426\n",
      "Epoch 37 [6400/60000] Loss: 0.122110 | Grad Norm: 0.058843\n",
      "Epoch 37 [12800/60000] Loss: 0.122584 | Grad Norm: 0.014094\n",
      "Epoch 37 [19200/60000] Loss: 0.123649 | Grad Norm: 0.172864\n",
      "Epoch 37 [25600/60000] Loss: 0.123229 | Grad Norm: 0.001646\n",
      "Epoch 37 [32000/60000] Loss: 0.124308 | Grad Norm: 0.000018\n",
      "Epoch 37 [38400/60000] Loss: 0.123783 | Grad Norm: 0.000057\n",
      "Epoch 37 [44800/60000] Loss: 0.124496 | Grad Norm: 0.000001\n",
      "Epoch 37 [51200/60000] Loss: 0.123488 | Grad Norm: 0.086461\n",
      "Epoch 37 [57600/60000] Loss: 0.123257 | Grad Norm: 0.000032\n",
      "Accuracy on test images: 86.05%\n",
      "Epoch 38 [0/60000] Loss: 0.062706 | Grad Norm: 0.022301\n",
      "Epoch 38 [6400/60000] Loss: 0.117547 | Grad Norm: 0.015715\n",
      "Epoch 38 [12800/60000] Loss: 0.121537 | Grad Norm: 0.000454\n",
      "Epoch 38 [19200/60000] Loss: 0.121361 | Grad Norm: 0.769332\n",
      "Epoch 38 [25600/60000] Loss: 0.123186 | Grad Norm: 0.003982\n",
      "Epoch 38 [32000/60000] Loss: 0.124112 | Grad Norm: 0.999999\n",
      "Epoch 38 [38400/60000] Loss: 0.123570 | Grad Norm: 0.000784\n",
      "Epoch 38 [44800/60000] Loss: 0.123442 | Grad Norm: 0.000523\n",
      "Epoch 38 [51200/60000] Loss: 0.123524 | Grad Norm: 0.014577\n",
      "Epoch 38 [57600/60000] Loss: 0.123337 | Grad Norm: 0.038444\n",
      "Accuracy on test images: 86.32%\n",
      "Epoch 39 [0/60000] Loss: 0.120289 | Grad Norm: 1.000000\n",
      "Epoch 39 [6400/60000] Loss: 0.122126 | Grad Norm: 0.212173\n",
      "Epoch 39 [12800/60000] Loss: 0.124026 | Grad Norm: 0.000948\n",
      "Epoch 39 [19200/60000] Loss: 0.122573 | Grad Norm: 0.001464\n",
      "Epoch 39 [25600/60000] Loss: 0.121511 | Grad Norm: 0.003872\n",
      "Epoch 39 [32000/60000] Loss: 0.120978 | Grad Norm: 0.047662\n",
      "Epoch 39 [38400/60000] Loss: 0.122467 | Grad Norm: 0.661881\n",
      "Epoch 39 [44800/60000] Loss: 0.122668 | Grad Norm: 0.337745\n",
      "Epoch 39 [51200/60000] Loss: 0.122460 | Grad Norm: 0.095898\n",
      "Epoch 39 [57600/60000] Loss: 0.122427 | Grad Norm: 0.234773\n",
      "Accuracy on test images: 86.27%\n",
      "Epoch 40 [0/60000] Loss: 0.062500 | Grad Norm: 0.000026\n",
      "Epoch 40 [6400/60000] Loss: 0.125444 | Grad Norm: 0.043981\n",
      "Epoch 40 [12800/60000] Loss: 0.120515 | Grad Norm: 0.168727\n",
      "Epoch 40 [19200/60000] Loss: 0.121230 | Grad Norm: 0.218414\n",
      "Epoch 40 [25600/60000] Loss: 0.120994 | Grad Norm: 0.057923\n",
      "Epoch 40 [32000/60000] Loss: 0.120958 | Grad Norm: 0.829948\n",
      "Epoch 40 [38400/60000] Loss: 0.121353 | Grad Norm: 0.250627\n",
      "Epoch 40 [44800/60000] Loss: 0.122153 | Grad Norm: 0.000007\n",
      "Epoch 40 [51200/60000] Loss: 0.122597 | Grad Norm: 0.002732\n",
      "Epoch 40 [57600/60000] Loss: 0.122657 | Grad Norm: 0.552461\n",
      "Accuracy on test images: 86.47%\n",
      "Epoch 41 [0/60000] Loss: 0.171794 | Grad Norm: 0.016187\n",
      "Epoch 41 [6400/60000] Loss: 0.122491 | Grad Norm: 0.000080\n",
      "Epoch 41 [12800/60000] Loss: 0.122057 | Grad Norm: 0.036558\n",
      "Epoch 41 [19200/60000] Loss: 0.122230 | Grad Norm: 0.735567\n",
      "Epoch 41 [25600/60000] Loss: 0.122588 | Grad Norm: 0.011571\n",
      "Epoch 41 [32000/60000] Loss: 0.122239 | Grad Norm: 0.000357\n",
      "Epoch 41 [38400/60000] Loss: 0.121366 | Grad Norm: 0.048298\n",
      "Epoch 41 [44800/60000] Loss: 0.121408 | Grad Norm: 0.004651\n",
      "Epoch 41 [51200/60000] Loss: 0.121483 | Grad Norm: 1.000000\n",
      "Epoch 41 [57600/60000] Loss: 0.121693 | Grad Norm: 0.001538\n",
      "Accuracy on test images: 86.29%\n",
      "Epoch 42 [0/60000] Loss: 0.125000 | Grad Norm: 0.000007\n",
      "Epoch 42 [6400/60000] Loss: 0.123307 | Grad Norm: 0.001621\n",
      "Epoch 42 [12800/60000] Loss: 0.123877 | Grad Norm: 0.001581\n",
      "Epoch 42 [19200/60000] Loss: 0.123915 | Grad Norm: 0.008316\n",
      "Epoch 42 [25600/60000] Loss: 0.122845 | Grad Norm: 0.082035\n",
      "Epoch 42 [32000/60000] Loss: 0.122151 | Grad Norm: 0.505622\n",
      "Epoch 42 [38400/60000] Loss: 0.121683 | Grad Norm: 0.731448\n",
      "Epoch 42 [44800/60000] Loss: 0.121944 | Grad Norm: 0.859720\n",
      "Epoch 42 [51200/60000] Loss: 0.121961 | Grad Norm: 0.040252\n",
      "Epoch 42 [57600/60000] Loss: 0.122620 | Grad Norm: 0.066945\n",
      "Accuracy on test images: 86.04%\n",
      "Epoch 43 [0/60000] Loss: 0.031930 | Grad Norm: 0.373145\n",
      "Epoch 43 [6400/60000] Loss: 0.111528 | Grad Norm: 0.552654\n",
      "Epoch 43 [12800/60000] Loss: 0.116291 | Grad Norm: 0.001468\n",
      "Epoch 43 [19200/60000] Loss: 0.120346 | Grad Norm: 0.241043\n",
      "Epoch 43 [25600/60000] Loss: 0.118689 | Grad Norm: 0.122002\n",
      "Epoch 43 [32000/60000] Loss: 0.117303 | Grad Norm: 0.761710\n",
      "Epoch 43 [38400/60000] Loss: 0.119020 | Grad Norm: 0.000617\n",
      "Epoch 43 [44800/60000] Loss: 0.120792 | Grad Norm: 0.101367\n",
      "Epoch 43 [51200/60000] Loss: 0.120549 | Grad Norm: 1.000000\n",
      "Epoch 43 [57600/60000] Loss: 0.121182 | Grad Norm: 0.000000\n",
      "Accuracy on test images: 86.35%\n",
      "Epoch 44 [0/60000] Loss: 0.104695 | Grad Norm: 0.999999\n",
      "Epoch 44 [6400/60000] Loss: 0.113863 | Grad Norm: 0.000004\n",
      "Epoch 44 [12800/60000] Loss: 0.116991 | Grad Norm: 0.085504\n",
      "Epoch 44 [19200/60000] Loss: 0.118970 | Grad Norm: 0.000023\n",
      "Epoch 44 [25600/60000] Loss: 0.120650 | Grad Norm: 0.033500\n",
      "Epoch 44 [32000/60000] Loss: 0.121058 | Grad Norm: 0.000068\n",
      "Epoch 44 [38400/60000] Loss: 0.120661 | Grad Norm: 0.000475\n",
      "Epoch 44 [44800/60000] Loss: 0.121310 | Grad Norm: 0.000036\n",
      "Epoch 44 [51200/60000] Loss: 0.121086 | Grad Norm: 0.000759\n",
      "Epoch 44 [57600/60000] Loss: 0.121095 | Grad Norm: 0.000401\n",
      "Accuracy on test images: 86.25%\n",
      "Epoch 45 [0/60000] Loss: 0.140847 | Grad Norm: 0.081263\n",
      "Epoch 45 [6400/60000] Loss: 0.118531 | Grad Norm: 0.000003\n",
      "Epoch 45 [12800/60000] Loss: 0.117661 | Grad Norm: 0.045538\n",
      "Epoch 45 [19200/60000] Loss: 0.117812 | Grad Norm: 0.999999\n",
      "Epoch 45 [25600/60000] Loss: 0.120151 | Grad Norm: 0.003168\n",
      "Epoch 45 [32000/60000] Loss: 0.118983 | Grad Norm: 0.447774\n",
      "Epoch 45 [38400/60000] Loss: 0.117267 | Grad Norm: 0.675077\n",
      "Epoch 45 [44800/60000] Loss: 0.114135 | Grad Norm: 0.246389\n",
      "Epoch 45 [51200/60000] Loss: 0.111579 | Grad Norm: 0.541885\n",
      "Epoch 45 [57600/60000] Loss: 0.109512 | Grad Norm: 0.042542\n",
      "Accuracy on test images: 88.97%\n",
      "Epoch 46 [0/60000] Loss: 0.138613 | Grad Norm: 0.138999\n",
      "Epoch 46 [6400/60000] Loss: 0.093540 | Grad Norm: 0.432045\n",
      "Epoch 46 [12800/60000] Loss: 0.093924 | Grad Norm: 0.263852\n",
      "Epoch 46 [19200/60000] Loss: 0.092065 | Grad Norm: 0.501375\n",
      "Epoch 46 [25600/60000] Loss: 0.089955 | Grad Norm: 0.018465\n",
      "Epoch 46 [32000/60000] Loss: 0.088612 | Grad Norm: 0.601403\n",
      "Epoch 46 [38400/60000] Loss: 0.089014 | Grad Norm: 0.390421\n",
      "Epoch 46 [44800/60000] Loss: 0.087416 | Grad Norm: 0.296920\n",
      "Epoch 46 [51200/60000] Loss: 0.087097 | Grad Norm: 0.093213\n",
      "Epoch 46 [57600/60000] Loss: 0.086094 | Grad Norm: 0.635437\n",
      "Accuracy on test images: 90.49%\n",
      "Epoch 47 [0/60000] Loss: 0.077235 | Grad Norm: 0.168790\n",
      "Epoch 47 [6400/60000] Loss: 0.071542 | Grad Norm: 0.668501\n",
      "Epoch 47 [12800/60000] Loss: 0.073862 | Grad Norm: 0.456773\n",
      "Epoch 47 [19200/60000] Loss: 0.072470 | Grad Norm: 0.999999\n",
      "Epoch 47 [25600/60000] Loss: 0.073363 | Grad Norm: 0.237337\n",
      "Epoch 47 [32000/60000] Loss: 0.073630 | Grad Norm: 0.999999\n",
      "Epoch 47 [38400/60000] Loss: 0.074408 | Grad Norm: 0.407779\n",
      "Epoch 47 [44800/60000] Loss: 0.075280 | Grad Norm: 0.809365\n",
      "Epoch 47 [51200/60000] Loss: 0.074640 | Grad Norm: 0.810514\n",
      "Epoch 47 [57600/60000] Loss: 0.075204 | Grad Norm: 0.429861\n",
      "Accuracy on test images: 91.12%\n",
      "Epoch 48 [0/60000] Loss: 0.102766 | Grad Norm: 0.613200\n",
      "Epoch 48 [6400/60000] Loss: 0.068674 | Grad Norm: 0.999999\n",
      "Epoch 48 [12800/60000] Loss: 0.069204 | Grad Norm: 0.617390\n",
      "Epoch 48 [19200/60000] Loss: 0.067314 | Grad Norm: 0.041780\n",
      "Epoch 48 [25600/60000] Loss: 0.069550 | Grad Norm: 0.578884\n",
      "Epoch 48 [32000/60000] Loss: 0.069763 | Grad Norm: 0.568594\n",
      "Epoch 48 [38400/60000] Loss: 0.068825 | Grad Norm: 0.005292\n",
      "Epoch 48 [44800/60000] Loss: 0.069403 | Grad Norm: 0.794852\n",
      "Epoch 48 [51200/60000] Loss: 0.069548 | Grad Norm: 0.752625\n",
      "Epoch 48 [57600/60000] Loss: 0.069282 | Grad Norm: 0.999999\n",
      "Accuracy on test images: 91.18%\n",
      "Epoch 49 [0/60000] Loss: 0.073759 | Grad Norm: 0.326117\n",
      "Epoch 49 [6400/60000] Loss: 0.063667 | Grad Norm: 0.805642\n",
      "Epoch 49 [12800/60000] Loss: 0.066457 | Grad Norm: 0.309552\n",
      "Epoch 49 [19200/60000] Loss: 0.065538 | Grad Norm: 0.171017\n",
      "Epoch 49 [25600/60000] Loss: 0.065860 | Grad Norm: 0.501595\n",
      "Epoch 49 [32000/60000] Loss: 0.064860 | Grad Norm: 0.668956\n",
      "Epoch 49 [38400/60000] Loss: 0.065093 | Grad Norm: 0.078805\n",
      "Epoch 49 [44800/60000] Loss: 0.065156 | Grad Norm: 0.600609\n",
      "Epoch 49 [51200/60000] Loss: 0.065242 | Grad Norm: 0.999999\n",
      "Epoch 49 [57600/60000] Loss: 0.065449 | Grad Norm: 0.589223\n",
      "Accuracy on test images: 91.35%\n",
      "Epoch 50 [0/60000] Loss: 0.054138 | Grad Norm: 0.307613\n",
      "Epoch 50 [6400/60000] Loss: 0.061008 | Grad Norm: 0.999999\n",
      "Epoch 50 [12800/60000] Loss: 0.062922 | Grad Norm: 0.180980\n",
      "Epoch 50 [19200/60000] Loss: 0.060821 | Grad Norm: 0.403607\n",
      "Epoch 50 [25600/60000] Loss: 0.061378 | Grad Norm: 0.837693\n",
      "Epoch 50 [32000/60000] Loss: 0.062077 | Grad Norm: 0.785935\n",
      "Epoch 50 [38400/60000] Loss: 0.061968 | Grad Norm: 0.434532\n",
      "Epoch 50 [44800/60000] Loss: 0.062419 | Grad Norm: 0.581712\n",
      "Epoch 50 [51200/60000] Loss: 0.062365 | Grad Norm: 0.967017\n",
      "Epoch 50 [57600/60000] Loss: 0.062682 | Grad Norm: 0.594511\n",
      "Accuracy on test images: 91.44%\n",
      "Epoch 51 [0/60000] Loss: 0.094870 | Grad Norm: 0.122236\n",
      "Epoch 51 [6400/60000] Loss: 0.064389 | Grad Norm: 0.496611\n",
      "Epoch 51 [12800/60000] Loss: 0.061592 | Grad Norm: 0.383126\n",
      "Epoch 51 [19200/60000] Loss: 0.060369 | Grad Norm: 1.000000\n",
      "Epoch 51 [25600/60000] Loss: 0.059764 | Grad Norm: 0.027656\n",
      "Epoch 51 [32000/60000] Loss: 0.059392 | Grad Norm: 0.018702\n",
      "Epoch 51 [38400/60000] Loss: 0.059875 | Grad Norm: 0.444748\n",
      "Epoch 51 [44800/60000] Loss: 0.059704 | Grad Norm: 0.999999\n",
      "Epoch 51 [51200/60000] Loss: 0.059579 | Grad Norm: 0.306202\n",
      "Epoch 51 [57600/60000] Loss: 0.059836 | Grad Norm: 0.999999\n",
      "Accuracy on test images: 91.30%\n",
      "Epoch 52 [0/60000] Loss: 0.017285 | Grad Norm: 0.168688\n",
      "Epoch 52 [6400/60000] Loss: 0.057691 | Grad Norm: 0.281250\n",
      "Epoch 52 [12800/60000] Loss: 0.059303 | Grad Norm: 0.268412\n",
      "Epoch 52 [19200/60000] Loss: 0.058178 | Grad Norm: 0.475144\n",
      "Epoch 52 [25600/60000] Loss: 0.057464 | Grad Norm: 0.459229\n",
      "Epoch 52 [32000/60000] Loss: 0.058167 | Grad Norm: 0.077603\n",
      "Epoch 52 [38400/60000] Loss: 0.057522 | Grad Norm: 0.999999\n",
      "Epoch 52 [44800/60000] Loss: 0.057495 | Grad Norm: 0.315234\n",
      "Epoch 52 [51200/60000] Loss: 0.057396 | Grad Norm: 0.999999\n",
      "Epoch 52 [57600/60000] Loss: 0.057533 | Grad Norm: 0.973105\n",
      "Accuracy on test images: 91.14%\n",
      "Epoch 53 [0/60000] Loss: 0.048178 | Grad Norm: 0.911228\n",
      "Epoch 53 [6400/60000] Loss: 0.054285 | Grad Norm: 0.462429\n",
      "Epoch 53 [12800/60000] Loss: 0.056551 | Grad Norm: 0.252506\n",
      "Epoch 53 [19200/60000] Loss: 0.056516 | Grad Norm: 0.326627\n",
      "Epoch 53 [25600/60000] Loss: 0.055687 | Grad Norm: 0.045728\n",
      "Epoch 53 [32000/60000] Loss: 0.056079 | Grad Norm: 0.128747\n",
      "Epoch 53 [38400/60000] Loss: 0.056213 | Grad Norm: 0.821441\n",
      "Epoch 53 [44800/60000] Loss: 0.055758 | Grad Norm: 0.081758\n",
      "Epoch 53 [51200/60000] Loss: 0.056587 | Grad Norm: 0.661423\n",
      "Epoch 53 [57600/60000] Loss: 0.056556 | Grad Norm: 0.025419\n",
      "Accuracy on test images: 92.10%\n",
      "Epoch 54 [0/60000] Loss: 0.057983 | Grad Norm: 0.764233\n",
      "Epoch 54 [6400/60000] Loss: 0.049748 | Grad Norm: 0.902659\n",
      "Epoch 54 [12800/60000] Loss: 0.051578 | Grad Norm: 0.172962\n",
      "Epoch 54 [19200/60000] Loss: 0.052041 | Grad Norm: 1.000000\n",
      "Epoch 54 [25600/60000] Loss: 0.052178 | Grad Norm: 0.317959\n",
      "Epoch 54 [32000/60000] Loss: 0.053175 | Grad Norm: 0.033446\n",
      "Epoch 54 [38400/60000] Loss: 0.053685 | Grad Norm: 0.447963\n",
      "Epoch 54 [44800/60000] Loss: 0.053877 | Grad Norm: 0.999999\n",
      "Epoch 54 [51200/60000] Loss: 0.054297 | Grad Norm: 0.242022\n",
      "Epoch 54 [57600/60000] Loss: 0.054792 | Grad Norm: 0.280687\n",
      "Accuracy on test images: 91.93%\n",
      "Epoch 55 [0/60000] Loss: 0.031254 | Grad Norm: 0.000516\n",
      "Epoch 55 [6400/60000] Loss: 0.045671 | Grad Norm: 0.420996\n",
      "Epoch 55 [12800/60000] Loss: 0.047702 | Grad Norm: 0.092207\n",
      "Epoch 55 [19200/60000] Loss: 0.049454 | Grad Norm: 0.315843\n",
      "Epoch 55 [25600/60000] Loss: 0.051460 | Grad Norm: 0.465177\n",
      "Epoch 55 [32000/60000] Loss: 0.051687 | Grad Norm: 0.499702\n",
      "Epoch 55 [38400/60000] Loss: 0.052227 | Grad Norm: 0.695931\n",
      "Epoch 55 [44800/60000] Loss: 0.051993 | Grad Norm: 0.442152\n",
      "Epoch 55 [51200/60000] Loss: 0.052534 | Grad Norm: 0.999999\n",
      "Epoch 55 [57600/60000] Loss: 0.052982 | Grad Norm: 0.002454\n",
      "Accuracy on test images: 91.89%\n",
      "Epoch 56 [0/60000] Loss: 0.062525 | Grad Norm: 0.001434\n",
      "Epoch 56 [6400/60000] Loss: 0.048569 | Grad Norm: 0.898736\n",
      "Epoch 56 [12800/60000] Loss: 0.049521 | Grad Norm: 1.000000\n",
      "Epoch 56 [19200/60000] Loss: 0.050046 | Grad Norm: 0.122112\n",
      "Epoch 56 [25600/60000] Loss: 0.050937 | Grad Norm: 0.295550\n",
      "Epoch 56 [32000/60000] Loss: 0.050653 | Grad Norm: 0.587329\n",
      "Epoch 56 [38400/60000] Loss: 0.050725 | Grad Norm: 0.055170\n",
      "Epoch 56 [44800/60000] Loss: 0.050884 | Grad Norm: 0.294659\n",
      "Epoch 56 [51200/60000] Loss: 0.050912 | Grad Norm: 0.324445\n",
      "Epoch 56 [57600/60000] Loss: 0.051331 | Grad Norm: 0.999999\n",
      "Accuracy on test images: 91.68%\n",
      "Epoch 57 [0/60000] Loss: 0.071383 | Grad Norm: 0.660502\n",
      "Epoch 57 [6400/60000] Loss: 0.049886 | Grad Norm: 0.251488\n",
      "Epoch 57 [12800/60000] Loss: 0.048876 | Grad Norm: 0.352824\n",
      "Epoch 57 [19200/60000] Loss: 0.050079 | Grad Norm: 0.000299\n",
      "Epoch 57 [25600/60000] Loss: 0.049903 | Grad Norm: 0.515127\n",
      "Epoch 57 [32000/60000] Loss: 0.049761 | Grad Norm: 0.999999\n",
      "Epoch 57 [38400/60000] Loss: 0.050174 | Grad Norm: 0.398440\n",
      "Epoch 57 [44800/60000] Loss: 0.050207 | Grad Norm: 0.393253\n",
      "Epoch 57 [51200/60000] Loss: 0.050502 | Grad Norm: 0.617767\n",
      "Epoch 57 [57600/60000] Loss: 0.050498 | Grad Norm: 0.999999\n",
      "Accuracy on test images: 91.80%\n",
      "Epoch 58 [0/60000] Loss: 0.039090 | Grad Norm: 0.999999\n",
      "Epoch 58 [6400/60000] Loss: 0.047509 | Grad Norm: 0.428178\n",
      "Epoch 58 [12800/60000] Loss: 0.047764 | Grad Norm: 0.999999\n",
      "Epoch 58 [19200/60000] Loss: 0.048564 | Grad Norm: 0.003204\n",
      "Epoch 58 [25600/60000] Loss: 0.047529 | Grad Norm: 0.375166\n",
      "Epoch 58 [32000/60000] Loss: 0.048255 | Grad Norm: 0.732900\n",
      "Epoch 58 [38400/60000] Loss: 0.048407 | Grad Norm: 0.703109\n",
      "Epoch 58 [44800/60000] Loss: 0.048371 | Grad Norm: 0.142503\n",
      "Epoch 58 [51200/60000] Loss: 0.048310 | Grad Norm: 0.386489\n",
      "Epoch 58 [57600/60000] Loss: 0.048714 | Grad Norm: 0.253768\n",
      "Accuracy on test images: 91.93%\n",
      "Epoch 59 [0/60000] Loss: 0.121426 | Grad Norm: 0.647985\n",
      "Epoch 59 [6400/60000] Loss: 0.049187 | Grad Norm: 0.915839\n",
      "Epoch 59 [12800/60000] Loss: 0.047524 | Grad Norm: 0.680750\n",
      "Epoch 59 [19200/60000] Loss: 0.046916 | Grad Norm: 0.322908\n",
      "Epoch 59 [25600/60000] Loss: 0.047878 | Grad Norm: 0.003185\n",
      "Epoch 59 [32000/60000] Loss: 0.048149 | Grad Norm: 0.999999\n",
      "Epoch 59 [38400/60000] Loss: 0.048315 | Grad Norm: 0.999999\n",
      "Epoch 59 [44800/60000] Loss: 0.048612 | Grad Norm: 0.522539\n",
      "Epoch 59 [51200/60000] Loss: 0.048240 | Grad Norm: 0.070362\n",
      "Epoch 59 [57600/60000] Loss: 0.048625 | Grad Norm: 0.794517\n",
      "Accuracy on test images: 92.00%\n",
      "Epoch 60 [0/60000] Loss: 0.047483 | Grad Norm: 0.060082\n",
      "Epoch 60 [6400/60000] Loss: 0.047121 | Grad Norm: 0.624600\n",
      "Epoch 60 [12800/60000] Loss: 0.045644 | Grad Norm: 0.019470\n",
      "Epoch 60 [19200/60000] Loss: 0.046308 | Grad Norm: 0.649840\n",
      "Epoch 60 [25600/60000] Loss: 0.046654 | Grad Norm: 0.037700\n",
      "Epoch 60 [32000/60000] Loss: 0.045865 | Grad Norm: 0.000735\n",
      "Epoch 60 [38400/60000] Loss: 0.046186 | Grad Norm: 0.413424\n",
      "Epoch 60 [44800/60000] Loss: 0.046585 | Grad Norm: 0.431039\n",
      "Epoch 60 [51200/60000] Loss: 0.047039 | Grad Norm: 0.015744\n",
      "Epoch 60 [57600/60000] Loss: 0.047873 | Grad Norm: 0.236523\n",
      "Accuracy on test images: 91.97%\n",
      "Epoch 61 [0/60000] Loss: 0.060694 | Grad Norm: 0.504032\n",
      "Epoch 61 [6400/60000] Loss: 0.043509 | Grad Norm: 0.963566\n",
      "Epoch 61 [12800/60000] Loss: 0.043639 | Grad Norm: 0.006179\n",
      "Epoch 61 [19200/60000] Loss: 0.044683 | Grad Norm: 0.074822\n",
      "Epoch 61 [25600/60000] Loss: 0.045042 | Grad Norm: 0.117973\n",
      "Epoch 61 [32000/60000] Loss: 0.045204 | Grad Norm: 0.014963\n",
      "Epoch 61 [38400/60000] Loss: 0.045383 | Grad Norm: 0.957278\n",
      "Epoch 61 [44800/60000] Loss: 0.045381 | Grad Norm: 0.481235\n",
      "Epoch 61 [51200/60000] Loss: 0.045277 | Grad Norm: 0.285981\n",
      "Epoch 61 [57600/60000] Loss: 0.045703 | Grad Norm: 0.570467\n",
      "Accuracy on test images: 92.04%\n",
      "Epoch 62 [0/60000] Loss: 0.079916 | Grad Norm: 0.464290\n",
      "Epoch 62 [6400/60000] Loss: 0.040000 | Grad Norm: 0.722963\n",
      "Epoch 62 [12800/60000] Loss: 0.041578 | Grad Norm: 0.002592\n",
      "Epoch 62 [19200/60000] Loss: 0.042064 | Grad Norm: 0.893947\n",
      "Epoch 62 [25600/60000] Loss: 0.044328 | Grad Norm: 0.025150\n",
      "Epoch 62 [32000/60000] Loss: 0.044461 | Grad Norm: 0.308188\n",
      "Epoch 62 [38400/60000] Loss: 0.045139 | Grad Norm: 0.999999\n",
      "Epoch 62 [44800/60000] Loss: 0.046221 | Grad Norm: 0.028068\n",
      "Epoch 62 [51200/60000] Loss: 0.046458 | Grad Norm: 1.000000\n",
      "Epoch 62 [57600/60000] Loss: 0.046589 | Grad Norm: 0.018666\n",
      "Accuracy on test images: 91.66%\n",
      "Epoch 63 [0/60000] Loss: 0.032440 | Grad Norm: 0.120355\n",
      "Epoch 63 [6400/60000] Loss: 0.040666 | Grad Norm: 0.825031\n",
      "Epoch 63 [12800/60000] Loss: 0.039516 | Grad Norm: 0.001116\n",
      "Epoch 63 [19200/60000] Loss: 0.041324 | Grad Norm: 0.070588\n",
      "Epoch 63 [25600/60000] Loss: 0.041946 | Grad Norm: 0.000129\n",
      "Epoch 63 [32000/60000] Loss: 0.041955 | Grad Norm: 0.178994\n",
      "Epoch 63 [38400/60000] Loss: 0.042549 | Grad Norm: 0.957261\n",
      "Epoch 63 [44800/60000] Loss: 0.043271 | Grad Norm: 0.587135\n",
      "Epoch 63 [51200/60000] Loss: 0.043962 | Grad Norm: 1.000000\n",
      "Epoch 63 [57600/60000] Loss: 0.044846 | Grad Norm: 0.039076\n",
      "Accuracy on test images: 91.77%\n",
      "Epoch 64 [0/60000] Loss: 0.053324 | Grad Norm: 0.776626\n",
      "Epoch 64 [6400/60000] Loss: 0.044295 | Grad Norm: 0.513488\n",
      "Epoch 64 [12800/60000] Loss: 0.041521 | Grad Norm: 0.323963\n",
      "Epoch 64 [19200/60000] Loss: 0.042613 | Grad Norm: 0.048611\n",
      "Epoch 64 [25600/60000] Loss: 0.042145 | Grad Norm: 0.116094\n",
      "Epoch 64 [32000/60000] Loss: 0.042396 | Grad Norm: 0.400830\n",
      "Epoch 64 [38400/60000] Loss: 0.042753 | Grad Norm: 0.822644\n",
      "Epoch 64 [44800/60000] Loss: 0.043150 | Grad Norm: 0.999999\n",
      "Epoch 64 [51200/60000] Loss: 0.043095 | Grad Norm: 0.999999\n",
      "Epoch 64 [57600/60000] Loss: 0.043437 | Grad Norm: 0.016075\n",
      "Accuracy on test images: 91.88%\n",
      "Epoch 65 [0/60000] Loss: 0.024671 | Grad Norm: 0.603473\n",
      "Epoch 65 [6400/60000] Loss: 0.041443 | Grad Norm: 0.186900\n",
      "Epoch 65 [12800/60000] Loss: 0.040092 | Grad Norm: 0.862827\n",
      "Epoch 65 [19200/60000] Loss: 0.042381 | Grad Norm: 0.632194\n",
      "Epoch 65 [25600/60000] Loss: 0.042508 | Grad Norm: 0.397266\n",
      "Epoch 65 [32000/60000] Loss: 0.043051 | Grad Norm: 0.005195\n",
      "Epoch 65 [38400/60000] Loss: 0.042930 | Grad Norm: 0.085547\n",
      "Epoch 65 [44800/60000] Loss: 0.042848 | Grad Norm: 0.016196\n",
      "Epoch 65 [51200/60000] Loss: 0.042257 | Grad Norm: 0.999999\n",
      "Epoch 65 [57600/60000] Loss: 0.042395 | Grad Norm: 0.430079\n",
      "Accuracy on test images: 91.88%\n",
      "Epoch 66 [0/60000] Loss: 0.066945 | Grad Norm: 0.426843\n",
      "Epoch 66 [6400/60000] Loss: 0.042465 | Grad Norm: 0.203014\n",
      "Epoch 66 [12800/60000] Loss: 0.045022 | Grad Norm: 0.058476\n",
      "Epoch 66 [19200/60000] Loss: 0.042813 | Grad Norm: 0.003096\n",
      "Epoch 66 [25600/60000] Loss: 0.042261 | Grad Norm: 0.043256\n",
      "Epoch 66 [32000/60000] Loss: 0.041441 | Grad Norm: 0.325082\n",
      "Epoch 66 [38400/60000] Loss: 0.041715 | Grad Norm: 0.999999\n",
      "Epoch 66 [44800/60000] Loss: 0.041764 | Grad Norm: 0.397857\n",
      "Epoch 66 [51200/60000] Loss: 0.041933 | Grad Norm: 0.537009\n",
      "Epoch 66 [57600/60000] Loss: 0.042427 | Grad Norm: 0.539211\n",
      "Accuracy on test images: 91.87%\n",
      "Epoch 67 [0/60000] Loss: 0.032938 | Grad Norm: 0.130659\n",
      "Epoch 67 [6400/60000] Loss: 0.039735 | Grad Norm: 0.012104\n",
      "Epoch 67 [12800/60000] Loss: 0.039855 | Grad Norm: 0.499942\n",
      "Epoch 67 [19200/60000] Loss: 0.040690 | Grad Norm: 0.482784\n",
      "Epoch 67 [25600/60000] Loss: 0.041796 | Grad Norm: 0.102401\n",
      "Epoch 67 [32000/60000] Loss: 0.040915 | Grad Norm: 0.999999\n",
      "Epoch 67 [38400/60000] Loss: 0.041346 | Grad Norm: 0.624533\n",
      "Epoch 67 [44800/60000] Loss: 0.041529 | Grad Norm: 0.073694\n",
      "Epoch 67 [51200/60000] Loss: 0.041691 | Grad Norm: 0.999999\n",
      "Epoch 67 [57600/60000] Loss: 0.041774 | Grad Norm: 0.587090\n",
      "Accuracy on test images: 92.13%\n",
      "Epoch 68 [0/60000] Loss: 0.015737 | Grad Norm: 0.023197\n",
      "Epoch 68 [6400/60000] Loss: 0.035522 | Grad Norm: 0.005344\n",
      "Epoch 68 [12800/60000] Loss: 0.037003 | Grad Norm: 0.259868\n",
      "Epoch 68 [19200/60000] Loss: 0.038535 | Grad Norm: 0.620824\n",
      "Epoch 68 [25600/60000] Loss: 0.039162 | Grad Norm: 0.999999\n",
      "Epoch 68 [32000/60000] Loss: 0.039651 | Grad Norm: 0.000361\n",
      "Epoch 68 [38400/60000] Loss: 0.039901 | Grad Norm: 0.045959\n",
      "Epoch 68 [44800/60000] Loss: 0.040625 | Grad Norm: 0.397274\n",
      "Epoch 68 [51200/60000] Loss: 0.040205 | Grad Norm: 0.103705\n",
      "Epoch 68 [57600/60000] Loss: 0.039478 | Grad Norm: 0.254376\n",
      "Accuracy on test images: 92.05%\n",
      "Epoch 69 [0/60000] Loss: 0.047195 | Grad Norm: 0.113344\n",
      "Epoch 69 [6400/60000] Loss: 0.033824 | Grad Norm: 0.249521\n",
      "Epoch 69 [12800/60000] Loss: 0.035995 | Grad Norm: 0.726842\n",
      "Epoch 69 [19200/60000] Loss: 0.037509 | Grad Norm: 0.027082\n",
      "Epoch 69 [25600/60000] Loss: 0.038043 | Grad Norm: 0.030290\n",
      "Epoch 69 [32000/60000] Loss: 0.037955 | Grad Norm: 0.000480\n",
      "Epoch 69 [38400/60000] Loss: 0.038575 | Grad Norm: 0.189717\n",
      "Epoch 69 [44800/60000] Loss: 0.038418 | Grad Norm: 0.340842\n",
      "Epoch 69 [51200/60000] Loss: 0.038919 | Grad Norm: 0.428578\n",
      "Epoch 69 [57600/60000] Loss: 0.039046 | Grad Norm: 0.011011\n",
      "Accuracy on test images: 92.12%\n",
      "Epoch 70 [0/60000] Loss: 0.031253 | Grad Norm: 0.000242\n",
      "Epoch 70 [6400/60000] Loss: 0.040794 | Grad Norm: 0.999999\n",
      "Epoch 70 [12800/60000] Loss: 0.040153 | Grad Norm: 0.507957\n",
      "Epoch 70 [19200/60000] Loss: 0.039760 | Grad Norm: 0.737126\n",
      "Epoch 70 [25600/60000] Loss: 0.039152 | Grad Norm: 0.003536\n",
      "Epoch 70 [32000/60000] Loss: 0.039058 | Grad Norm: 0.240268\n",
      "Epoch 70 [38400/60000] Loss: 0.038773 | Grad Norm: 0.384226\n",
      "Epoch 70 [44800/60000] Loss: 0.038384 | Grad Norm: 0.446869\n",
      "Epoch 70 [51200/60000] Loss: 0.038368 | Grad Norm: 0.575011\n",
      "Epoch 70 [57600/60000] Loss: 0.038558 | Grad Norm: 0.000420\n",
      "Accuracy on test images: 92.07%\n",
      "Epoch 71 [0/60000] Loss: 0.035851 | Grad Norm: 0.999999\n",
      "Epoch 71 [6400/60000] Loss: 0.039200 | Grad Norm: 0.168985\n",
      "Epoch 71 [12800/60000] Loss: 0.035745 | Grad Norm: 0.000023\n",
      "Epoch 71 [19200/60000] Loss: 0.037879 | Grad Norm: 0.088504\n",
      "Epoch 71 [25600/60000] Loss: 0.038413 | Grad Norm: 0.722971\n",
      "Epoch 71 [32000/60000] Loss: 0.039122 | Grad Norm: 0.187186\n",
      "Epoch 71 [38400/60000] Loss: 0.038996 | Grad Norm: 0.238893\n",
      "Epoch 71 [44800/60000] Loss: 0.038773 | Grad Norm: 0.031284\n",
      "Epoch 71 [51200/60000] Loss: 0.039120 | Grad Norm: 0.061428\n",
      "Epoch 71 [57600/60000] Loss: 0.038975 | Grad Norm: 0.522103\n",
      "Accuracy on test images: 92.33%\n",
      "Epoch 72 [0/60000] Loss: 0.092636 | Grad Norm: 0.999999\n",
      "Epoch 72 [6400/60000] Loss: 0.039474 | Grad Norm: 0.000141\n",
      "Epoch 72 [12800/60000] Loss: 0.038875 | Grad Norm: 0.000005\n",
      "Epoch 72 [19200/60000] Loss: 0.036666 | Grad Norm: 0.000477\n",
      "Epoch 72 [25600/60000] Loss: 0.035835 | Grad Norm: 0.002385\n",
      "Epoch 72 [32000/60000] Loss: 0.036044 | Grad Norm: 0.012648\n",
      "Epoch 72 [38400/60000] Loss: 0.036303 | Grad Norm: 0.034589\n",
      "Epoch 72 [44800/60000] Loss: 0.036550 | Grad Norm: 0.445557\n",
      "Epoch 72 [51200/60000] Loss: 0.036782 | Grad Norm: 0.948026\n",
      "Epoch 72 [57600/60000] Loss: 0.037113 | Grad Norm: 0.545214\n",
      "Accuracy on test images: 92.21%\n",
      "Epoch 73 [0/60000] Loss: 0.049081 | Grad Norm: 0.294569\n",
      "Epoch 73 [6400/60000] Loss: 0.036561 | Grad Norm: 0.300255\n",
      "Epoch 73 [12800/60000] Loss: 0.036675 | Grad Norm: 1.000000\n",
      "Epoch 73 [19200/60000] Loss: 0.038626 | Grad Norm: 0.001895\n",
      "Epoch 73 [25600/60000] Loss: 0.039337 | Grad Norm: 0.366512\n",
      "Epoch 73 [32000/60000] Loss: 0.039245 | Grad Norm: 0.000668\n",
      "Epoch 73 [38400/60000] Loss: 0.038817 | Grad Norm: 0.999999\n",
      "Epoch 73 [44800/60000] Loss: 0.038702 | Grad Norm: 0.007098\n",
      "Epoch 73 [51200/60000] Loss: 0.038894 | Grad Norm: 0.531416\n",
      "Epoch 73 [57600/60000] Loss: 0.038458 | Grad Norm: 0.292077\n",
      "Accuracy on test images: 92.37%\n",
      "Epoch 74 [0/60000] Loss: 0.018158 | Grad Norm: 0.439134\n",
      "Epoch 74 [6400/60000] Loss: 0.037539 | Grad Norm: 0.592259\n",
      "Epoch 74 [12800/60000] Loss: 0.037322 | Grad Norm: 0.017206\n",
      "Epoch 74 [19200/60000] Loss: 0.036067 | Grad Norm: 0.498865\n",
      "Epoch 74 [25600/60000] Loss: 0.036904 | Grad Norm: 0.391177\n",
      "Epoch 74 [32000/60000] Loss: 0.036765 | Grad Norm: 0.385054\n",
      "Epoch 74 [38400/60000] Loss: 0.036632 | Grad Norm: 0.873852\n",
      "Epoch 74 [44800/60000] Loss: 0.036636 | Grad Norm: 0.342916\n",
      "Epoch 74 [51200/60000] Loss: 0.036630 | Grad Norm: 0.011775\n",
      "Epoch 74 [57600/60000] Loss: 0.036605 | Grad Norm: 0.065901\n",
      "Accuracy on test images: 92.31%\n",
      "Epoch 75 [0/60000] Loss: 0.011857 | Grad Norm: 0.653658\n",
      "Epoch 75 [6400/60000] Loss: 0.033356 | Grad Norm: 0.379593\n",
      "Epoch 75 [12800/60000] Loss: 0.030977 | Grad Norm: 0.000949\n",
      "Epoch 75 [19200/60000] Loss: 0.032025 | Grad Norm: 0.000677\n",
      "Epoch 75 [25600/60000] Loss: 0.034114 | Grad Norm: 0.684611\n",
      "Epoch 75 [32000/60000] Loss: 0.034213 | Grad Norm: 0.999999\n",
      "Epoch 75 [38400/60000] Loss: 0.036028 | Grad Norm: 0.511558\n",
      "Epoch 75 [44800/60000] Loss: 0.036203 | Grad Norm: 0.000620\n",
      "Epoch 75 [51200/60000] Loss: 0.036078 | Grad Norm: 0.612089\n",
      "Epoch 75 [57600/60000] Loss: 0.035760 | Grad Norm: 0.258099\n",
      "Accuracy on test images: 92.34%\n",
      "Epoch 76 [0/60000] Loss: 0.082868 | Grad Norm: 0.614625\n",
      "Epoch 76 [6400/60000] Loss: 0.035573 | Grad Norm: 0.737769\n",
      "Epoch 76 [12800/60000] Loss: 0.034821 | Grad Norm: 0.002967\n",
      "Epoch 76 [19200/60000] Loss: 0.033906 | Grad Norm: 0.516984\n",
      "Epoch 76 [25600/60000] Loss: 0.033306 | Grad Norm: 0.000001\n",
      "Epoch 76 [32000/60000] Loss: 0.034127 | Grad Norm: 0.999999\n",
      "Epoch 76 [38400/60000] Loss: 0.035208 | Grad Norm: 0.000108\n",
      "Epoch 76 [44800/60000] Loss: 0.036098 | Grad Norm: 0.260954\n",
      "Epoch 76 [51200/60000] Loss: 0.036175 | Grad Norm: 0.677527\n",
      "Epoch 76 [57600/60000] Loss: 0.036151 | Grad Norm: 0.046427\n",
      "Accuracy on test images: 92.14%\n",
      "Epoch 77 [0/60000] Loss: 0.031250 | Grad Norm: 0.000046\n",
      "Epoch 77 [6400/60000] Loss: 0.035759 | Grad Norm: 0.069918\n",
      "Epoch 77 [12800/60000] Loss: 0.034859 | Grad Norm: 0.069585\n",
      "Epoch 77 [19200/60000] Loss: 0.034585 | Grad Norm: 0.852336\n",
      "Epoch 77 [25600/60000] Loss: 0.035224 | Grad Norm: 0.078477\n",
      "Epoch 77 [32000/60000] Loss: 0.034646 | Grad Norm: 0.982152\n",
      "Epoch 77 [38400/60000] Loss: 0.035527 | Grad Norm: 0.004707\n",
      "Epoch 77 [44800/60000] Loss: 0.035483 | Grad Norm: 0.000018\n",
      "Epoch 77 [51200/60000] Loss: 0.035329 | Grad Norm: 1.000000\n",
      "Epoch 77 [57600/60000] Loss: 0.035037 | Grad Norm: 0.000131\n",
      "Accuracy on test images: 92.42%\n",
      "Epoch 78 [0/60000] Loss: 0.000000 | Grad Norm: 0.000020\n",
      "Epoch 78 [6400/60000] Loss: 0.035923 | Grad Norm: 0.001143\n",
      "Epoch 78 [12800/60000] Loss: 0.033059 | Grad Norm: 0.999999\n",
      "Epoch 78 [19200/60000] Loss: 0.032281 | Grad Norm: 0.999999\n",
      "Epoch 78 [25600/60000] Loss: 0.032728 | Grad Norm: 0.821565\n",
      "Epoch 78 [32000/60000] Loss: 0.032864 | Grad Norm: 0.002197\n",
      "Epoch 78 [38400/60000] Loss: 0.032851 | Grad Norm: 0.004488\n",
      "Epoch 78 [44800/60000] Loss: 0.033334 | Grad Norm: 0.381989\n",
      "Epoch 78 [51200/60000] Loss: 0.033804 | Grad Norm: 0.477588\n",
      "Epoch 78 [57600/60000] Loss: 0.034686 | Grad Norm: 0.995927\n",
      "Accuracy on test images: 92.22%\n",
      "Epoch 79 [0/60000] Loss: 0.031675 | Grad Norm: 0.103705\n",
      "Epoch 79 [6400/60000] Loss: 0.030653 | Grad Norm: 0.098938\n",
      "Epoch 79 [12800/60000] Loss: 0.032730 | Grad Norm: 0.280884\n",
      "Epoch 79 [19200/60000] Loss: 0.032796 | Grad Norm: 0.205312\n",
      "Epoch 79 [25600/60000] Loss: 0.033429 | Grad Norm: 0.512961\n",
      "Epoch 79 [32000/60000] Loss: 0.033204 | Grad Norm: 0.624402\n",
      "Epoch 79 [38400/60000] Loss: 0.033258 | Grad Norm: 0.051482\n",
      "Epoch 79 [44800/60000] Loss: 0.034063 | Grad Norm: 0.543545\n",
      "Epoch 79 [51200/60000] Loss: 0.033923 | Grad Norm: 0.157334\n",
      "Epoch 79 [57600/60000] Loss: 0.034271 | Grad Norm: 0.739352\n",
      "Accuracy on test images: 91.95%\n",
      "Epoch 80 [0/60000] Loss: 0.033173 | Grad Norm: 0.297837\n",
      "Epoch 80 [6400/60000] Loss: 0.030992 | Grad Norm: 0.024640\n",
      "Epoch 80 [12800/60000] Loss: 0.030918 | Grad Norm: 0.601701\n",
      "Epoch 80 [19200/60000] Loss: 0.030852 | Grad Norm: 0.009969\n",
      "Epoch 80 [25600/60000] Loss: 0.031638 | Grad Norm: 0.000286\n",
      "Epoch 80 [32000/60000] Loss: 0.033134 | Grad Norm: 0.109521\n",
      "Epoch 80 [38400/60000] Loss: 0.032767 | Grad Norm: 0.017058\n",
      "Epoch 80 [44800/60000] Loss: 0.033318 | Grad Norm: 0.197721\n",
      "Epoch 80 [51200/60000] Loss: 0.033344 | Grad Norm: 0.649072\n",
      "Epoch 80 [57600/60000] Loss: 0.033862 | Grad Norm: 0.009019\n",
      "Accuracy on test images: 92.06%\n",
      "Epoch 81 [0/60000] Loss: 0.047008 | Grad Norm: 0.109447\n",
      "Epoch 81 [6400/60000] Loss: 0.030942 | Grad Norm: 0.027355\n",
      "Epoch 81 [12800/60000] Loss: 0.032171 | Grad Norm: 0.758586\n",
      "Epoch 81 [19200/60000] Loss: 0.031503 | Grad Norm: 0.000709\n",
      "Epoch 81 [25600/60000] Loss: 0.031974 | Grad Norm: 0.000026\n",
      "Epoch 81 [32000/60000] Loss: 0.032658 | Grad Norm: 0.026948\n",
      "Epoch 81 [38400/60000] Loss: 0.032960 | Grad Norm: 0.215227\n",
      "Epoch 81 [44800/60000] Loss: 0.033635 | Grad Norm: 0.000004\n",
      "Epoch 81 [51200/60000] Loss: 0.033685 | Grad Norm: 0.760443\n",
      "Epoch 81 [57600/60000] Loss: 0.033911 | Grad Norm: 0.999999\n",
      "Accuracy on test images: 92.38%\n",
      "Epoch 82 [0/60000] Loss: 0.015951 | Grad Norm: 0.036929\n",
      "Epoch 82 [6400/60000] Loss: 0.030907 | Grad Norm: 0.890184\n",
      "Epoch 82 [12800/60000] Loss: 0.031957 | Grad Norm: 0.676321\n",
      "Epoch 82 [19200/60000] Loss: 0.031142 | Grad Norm: 0.029229\n",
      "Epoch 82 [25600/60000] Loss: 0.031131 | Grad Norm: 0.000357\n",
      "Epoch 82 [32000/60000] Loss: 0.030980 | Grad Norm: 0.077452\n",
      "Epoch 82 [38400/60000] Loss: 0.031858 | Grad Norm: 0.005099\n",
      "Epoch 82 [44800/60000] Loss: 0.031826 | Grad Norm: 0.000935\n",
      "Epoch 82 [51200/60000] Loss: 0.031874 | Grad Norm: 0.082913\n",
      "Epoch 82 [57600/60000] Loss: 0.032224 | Grad Norm: 0.503964\n",
      "Accuracy on test images: 92.43%\n",
      "Epoch 83 [0/60000] Loss: 0.015736 | Grad Norm: 0.010855\n",
      "Epoch 83 [6400/60000] Loss: 0.029900 | Grad Norm: 0.485413\n",
      "Epoch 83 [12800/60000] Loss: 0.031637 | Grad Norm: 0.044956\n",
      "Epoch 83 [19200/60000] Loss: 0.031216 | Grad Norm: 0.000003\n",
      "Epoch 83 [25600/60000] Loss: 0.032323 | Grad Norm: 0.602314\n",
      "Epoch 83 [32000/60000] Loss: 0.032163 | Grad Norm: 0.975575\n",
      "Epoch 83 [38400/60000] Loss: 0.032195 | Grad Norm: 0.152007\n",
      "Epoch 83 [44800/60000] Loss: 0.032417 | Grad Norm: 0.358376\n",
      "Epoch 83 [51200/60000] Loss: 0.032155 | Grad Norm: 0.104645\n",
      "Epoch 83 [57600/60000] Loss: 0.031863 | Grad Norm: 0.038896\n",
      "Accuracy on test images: 92.43%\n",
      "Epoch 84 [0/60000] Loss: 0.018677 | Grad Norm: 0.372197\n",
      "Epoch 84 [6400/60000] Loss: 0.027948 | Grad Norm: 0.016330\n",
      "Epoch 84 [12800/60000] Loss: 0.032442 | Grad Norm: 0.007301\n",
      "Epoch 84 [19200/60000] Loss: 0.032283 | Grad Norm: 0.000002\n",
      "Epoch 84 [25600/60000] Loss: 0.032605 | Grad Norm: 0.999999\n",
      "Epoch 84 [32000/60000] Loss: 0.032440 | Grad Norm: 1.000000\n",
      "Epoch 84 [38400/60000] Loss: 0.032208 | Grad Norm: 0.139614\n",
      "Epoch 84 [44800/60000] Loss: 0.031980 | Grad Norm: 0.999999\n",
      "Epoch 84 [51200/60000] Loss: 0.032463 | Grad Norm: 0.159560\n",
      "Epoch 84 [57600/60000] Loss: 0.032401 | Grad Norm: 0.001162\n",
      "Accuracy on test images: 92.48%\n",
      "Epoch 85 [0/60000] Loss: 0.000152 | Grad Norm: 0.018002\n",
      "Epoch 85 [6400/60000] Loss: 0.030126 | Grad Norm: 0.510093\n",
      "Epoch 85 [12800/60000] Loss: 0.028814 | Grad Norm: 0.000004\n",
      "Epoch 85 [19200/60000] Loss: 0.028285 | Grad Norm: 0.999999\n",
      "Epoch 85 [25600/60000] Loss: 0.029293 | Grad Norm: 0.888861\n",
      "Epoch 85 [32000/60000] Loss: 0.029610 | Grad Norm: 0.012140\n",
      "Epoch 85 [38400/60000] Loss: 0.030160 | Grad Norm: 0.018764\n",
      "Epoch 85 [44800/60000] Loss: 0.029977 | Grad Norm: 0.525332\n",
      "Epoch 85 [51200/60000] Loss: 0.029865 | Grad Norm: 0.004550\n",
      "Epoch 85 [57600/60000] Loss: 0.030439 | Grad Norm: 0.010649\n",
      "Accuracy on test images: 91.95%\n",
      "Epoch 86 [0/60000] Loss: 0.015617 | Grad Norm: 0.003153\n",
      "Epoch 86 [6400/60000] Loss: 0.032298 | Grad Norm: 0.848037\n",
      "Epoch 86 [12800/60000] Loss: 0.030584 | Grad Norm: 0.171548\n",
      "Epoch 86 [19200/60000] Loss: 0.030796 | Grad Norm: 0.183802\n",
      "Epoch 86 [25600/60000] Loss: 0.030965 | Grad Norm: 0.942796\n",
      "Epoch 86 [32000/60000] Loss: 0.031171 | Grad Norm: 0.000035\n",
      "Epoch 86 [38400/60000] Loss: 0.030927 | Grad Norm: 0.000000\n",
      "Epoch 86 [44800/60000] Loss: 0.030683 | Grad Norm: 0.001363\n",
      "Epoch 86 [51200/60000] Loss: 0.030878 | Grad Norm: 0.014590\n",
      "Epoch 86 [57600/60000] Loss: 0.031131 | Grad Norm: 0.000816\n",
      "Accuracy on test images: 92.63%\n",
      "Epoch 87 [0/60000] Loss: 0.044757 | Grad Norm: 0.633665\n",
      "Epoch 87 [6400/60000] Loss: 0.029967 | Grad Norm: 0.800052\n",
      "Epoch 87 [12800/60000] Loss: 0.030879 | Grad Norm: 0.004131\n",
      "Epoch 87 [19200/60000] Loss: 0.030905 | Grad Norm: 1.000000\n",
      "Epoch 87 [25600/60000] Loss: 0.030945 | Grad Norm: 0.918534\n",
      "Epoch 87 [32000/60000] Loss: 0.031226 | Grad Norm: 0.352833\n",
      "Epoch 87 [38400/60000] Loss: 0.030662 | Grad Norm: 0.007217\n",
      "Epoch 87 [44800/60000] Loss: 0.031127 | Grad Norm: 0.313628\n",
      "Epoch 87 [51200/60000] Loss: 0.031530 | Grad Norm: 0.002605\n",
      "Epoch 87 [57600/60000] Loss: 0.031545 | Grad Norm: 0.462838\n",
      "Accuracy on test images: 92.14%\n",
      "Epoch 88 [0/60000] Loss: 0.015636 | Grad Norm: 0.002077\n",
      "Epoch 88 [6400/60000] Loss: 0.027672 | Grad Norm: 0.999999\n",
      "Epoch 88 [12800/60000] Loss: 0.030182 | Grad Norm: 0.016769\n",
      "Epoch 88 [19200/60000] Loss: 0.030505 | Grad Norm: 0.007966\n",
      "Epoch 88 [25600/60000] Loss: 0.031672 | Grad Norm: 0.600376\n",
      "Epoch 88 [32000/60000] Loss: 0.031737 | Grad Norm: 0.536491\n",
      "Epoch 88 [38400/60000] Loss: 0.031629 | Grad Norm: 0.670027\n",
      "Epoch 88 [44800/60000] Loss: 0.031193 | Grad Norm: 0.063592\n",
      "Epoch 88 [51200/60000] Loss: 0.031370 | Grad Norm: 0.004195\n",
      "Epoch 88 [57600/60000] Loss: 0.031418 | Grad Norm: 0.021496\n",
      "Accuracy on test images: 92.22%\n",
      "Epoch 89 [0/60000] Loss: 0.014556 | Grad Norm: 0.625299\n",
      "Epoch 89 [6400/60000] Loss: 0.032118 | Grad Norm: 0.596816\n",
      "Epoch 89 [12800/60000] Loss: 0.030729 | Grad Norm: 0.353295\n",
      "Epoch 89 [19200/60000] Loss: 0.030587 | Grad Norm: 0.000152\n",
      "Epoch 89 [25600/60000] Loss: 0.030647 | Grad Norm: 0.007060\n",
      "Epoch 89 [32000/60000] Loss: 0.030626 | Grad Norm: 0.067753\n",
      "Epoch 89 [38400/60000] Loss: 0.030806 | Grad Norm: 0.003261\n",
      "Epoch 89 [44800/60000] Loss: 0.030602 | Grad Norm: 0.014370\n",
      "Epoch 89 [51200/60000] Loss: 0.030780 | Grad Norm: 0.006027\n",
      "Epoch 89 [57600/60000] Loss: 0.031231 | Grad Norm: 0.000032\n",
      "Accuracy on test images: 92.19%\n",
      "Epoch 90 [0/60000] Loss: 0.004421 | Grad Norm: 0.671388\n",
      "Epoch 90 [6400/60000] Loss: 0.027976 | Grad Norm: 0.315203\n",
      "Epoch 90 [12800/60000] Loss: 0.029154 | Grad Norm: 0.127801\n",
      "Epoch 90 [19200/60000] Loss: 0.030457 | Grad Norm: 0.573174\n",
      "Epoch 90 [25600/60000] Loss: 0.030728 | Grad Norm: 0.018748\n",
      "Epoch 90 [32000/60000] Loss: 0.030708 | Grad Norm: 0.000017\n",
      "Epoch 90 [38400/60000] Loss: 0.030923 | Grad Norm: 0.000001\n",
      "Epoch 90 [44800/60000] Loss: 0.030627 | Grad Norm: 0.005352\n",
      "Epoch 90 [51200/60000] Loss: 0.030213 | Grad Norm: 0.013825\n",
      "Epoch 90 [57600/60000] Loss: 0.030431 | Grad Norm: 0.179366\n",
      "Accuracy on test images: 92.40%\n",
      "Epoch 91 [0/60000] Loss: 0.016290 | Grad Norm: 0.256585\n",
      "Epoch 91 [6400/60000] Loss: 0.033036 | Grad Norm: 0.999999\n",
      "Epoch 91 [12800/60000] Loss: 0.033113 | Grad Norm: 0.230231\n",
      "Epoch 91 [19200/60000] Loss: 0.032662 | Grad Norm: 0.000000\n",
      "Epoch 91 [25600/60000] Loss: 0.030799 | Grad Norm: 0.519841\n",
      "Epoch 91 [32000/60000] Loss: 0.030714 | Grad Norm: 0.999999\n",
      "Epoch 91 [38400/60000] Loss: 0.030490 | Grad Norm: 0.781826\n",
      "Epoch 91 [44800/60000] Loss: 0.030538 | Grad Norm: 0.999999\n",
      "Epoch 91 [51200/60000] Loss: 0.030552 | Grad Norm: 0.226498\n",
      "Epoch 91 [57600/60000] Loss: 0.030115 | Grad Norm: 0.025321\n",
      "Accuracy on test images: 92.11%\n",
      "Epoch 92 [0/60000] Loss: 0.051316 | Grad Norm: 0.462516\n",
      "Epoch 92 [6400/60000] Loss: 0.028809 | Grad Norm: 0.267611\n",
      "Epoch 92 [12800/60000] Loss: 0.028545 | Grad Norm: 0.857203\n",
      "Epoch 92 [19200/60000] Loss: 0.028676 | Grad Norm: 0.011426\n",
      "Epoch 92 [25600/60000] Loss: 0.029686 | Grad Norm: 0.092560\n",
      "Epoch 92 [32000/60000] Loss: 0.028913 | Grad Norm: 0.000002\n",
      "Epoch 92 [38400/60000] Loss: 0.030040 | Grad Norm: 0.087678\n",
      "Epoch 92 [44800/60000] Loss: 0.030122 | Grad Norm: 0.245673\n",
      "Epoch 92 [51200/60000] Loss: 0.029722 | Grad Norm: 0.651446\n",
      "Epoch 92 [57600/60000] Loss: 0.029693 | Grad Norm: 0.000022\n",
      "Accuracy on test images: 92.58%\n",
      "Epoch 93 [0/60000] Loss: 0.007598 | Grad Norm: 0.463085\n",
      "Epoch 93 [6400/60000] Loss: 0.027103 | Grad Norm: 0.021819\n",
      "Epoch 93 [12800/60000] Loss: 0.031386 | Grad Norm: 0.043743\n",
      "Epoch 93 [19200/60000] Loss: 0.030210 | Grad Norm: 0.000027\n",
      "Epoch 93 [25600/60000] Loss: 0.029730 | Grad Norm: 0.072859\n",
      "Epoch 93 [32000/60000] Loss: 0.030244 | Grad Norm: 0.000012\n",
      "Epoch 93 [38400/60000] Loss: 0.029337 | Grad Norm: 0.001221\n",
      "Epoch 93 [44800/60000] Loss: 0.029855 | Grad Norm: 1.000000\n",
      "Epoch 93 [51200/60000] Loss: 0.029823 | Grad Norm: 0.731667\n",
      "Epoch 93 [57600/60000] Loss: 0.030013 | Grad Norm: 0.000106\n",
      "Accuracy on test images: 92.62%\n",
      "Epoch 94 [0/60000] Loss: 0.015625 | Grad Norm: 0.000004\n",
      "Epoch 94 [6400/60000] Loss: 0.029282 | Grad Norm: 0.019019\n",
      "Epoch 94 [12800/60000] Loss: 0.029763 | Grad Norm: 0.239695\n",
      "Epoch 94 [19200/60000] Loss: 0.029688 | Grad Norm: 1.000000\n",
      "Epoch 94 [25600/60000] Loss: 0.030127 | Grad Norm: 0.028660\n",
      "Epoch 94 [32000/60000] Loss: 0.029376 | Grad Norm: 0.449640\n",
      "Epoch 94 [38400/60000] Loss: 0.029120 | Grad Norm: 0.004582\n",
      "Epoch 94 [44800/60000] Loss: 0.028601 | Grad Norm: 0.000524\n",
      "Epoch 94 [51200/60000] Loss: 0.028325 | Grad Norm: 0.226169\n",
      "Epoch 94 [57600/60000] Loss: 0.028784 | Grad Norm: 0.000103\n",
      "Accuracy on test images: 92.40%\n",
      "Epoch 95 [0/60000] Loss: 0.023690 | Grad Norm: 1.000000\n",
      "Epoch 95 [6400/60000] Loss: 0.027261 | Grad Norm: 0.016428\n",
      "Epoch 95 [12800/60000] Loss: 0.028150 | Grad Norm: 0.717694\n",
      "Epoch 95 [19200/60000] Loss: 0.029350 | Grad Norm: 0.002805\n",
      "Epoch 95 [25600/60000] Loss: 0.028313 | Grad Norm: 0.111651\n",
      "Epoch 95 [32000/60000] Loss: 0.027774 | Grad Norm: 0.555665\n",
      "Epoch 95 [38400/60000] Loss: 0.027804 | Grad Norm: 0.010017\n",
      "Epoch 95 [44800/60000] Loss: 0.028277 | Grad Norm: 0.014483\n",
      "Epoch 95 [51200/60000] Loss: 0.028647 | Grad Norm: 0.000103\n",
      "Epoch 95 [57600/60000] Loss: 0.028334 | Grad Norm: 0.003044\n",
      "Accuracy on test images: 92.16%\n",
      "Epoch 96 [0/60000] Loss: 0.034167 | Grad Norm: 0.798725\n",
      "Epoch 96 [6400/60000] Loss: 0.027038 | Grad Norm: 0.187791\n",
      "Epoch 96 [12800/60000] Loss: 0.027600 | Grad Norm: 0.342886\n",
      "Epoch 96 [19200/60000] Loss: 0.026890 | Grad Norm: 0.894083\n",
      "Epoch 96 [25600/60000] Loss: 0.027609 | Grad Norm: 0.467200\n",
      "Epoch 96 [32000/60000] Loss: 0.027744 | Grad Norm: 0.002794\n",
      "Epoch 96 [38400/60000] Loss: 0.027914 | Grad Norm: 0.253258\n",
      "Epoch 96 [44800/60000] Loss: 0.028180 | Grad Norm: 0.002180\n",
      "Epoch 96 [51200/60000] Loss: 0.028468 | Grad Norm: 0.999999\n",
      "Epoch 96 [57600/60000] Loss: 0.028399 | Grad Norm: 0.000018\n",
      "Accuracy on test images: 92.43%\n",
      "Epoch 97 [0/60000] Loss: 0.003895 | Grad Norm: 0.541369\n",
      "Epoch 97 [6400/60000] Loss: 0.027749 | Grad Norm: 0.458815\n",
      "Epoch 97 [12800/60000] Loss: 0.029213 | Grad Norm: 0.000750\n",
      "Epoch 97 [19200/60000] Loss: 0.029009 | Grad Norm: 0.999999\n",
      "Epoch 97 [25600/60000] Loss: 0.029480 | Grad Norm: 0.000802\n",
      "Epoch 97 [32000/60000] Loss: 0.028587 | Grad Norm: 0.060149\n",
      "Epoch 97 [38400/60000] Loss: 0.028338 | Grad Norm: 1.000000\n",
      "Epoch 97 [44800/60000] Loss: 0.028226 | Grad Norm: 0.645952\n",
      "Epoch 97 [51200/60000] Loss: 0.028474 | Grad Norm: 0.372182\n",
      "Epoch 97 [57600/60000] Loss: 0.028263 | Grad Norm: 0.021054\n",
      "Accuracy on test images: 92.26%\n",
      "Epoch 98 [0/60000] Loss: 0.000004 | Grad Norm: 0.000499\n",
      "Epoch 98 [6400/60000] Loss: 0.027900 | Grad Norm: 0.136877\n",
      "Epoch 98 [12800/60000] Loss: 0.028183 | Grad Norm: 0.999999\n",
      "Epoch 98 [19200/60000] Loss: 0.027587 | Grad Norm: 0.001659\n",
      "Epoch 98 [25600/60000] Loss: 0.026851 | Grad Norm: 0.048679\n",
      "Epoch 98 [32000/60000] Loss: 0.027332 | Grad Norm: 0.999999\n",
      "Epoch 98 [38400/60000] Loss: 0.027522 | Grad Norm: 0.910885\n",
      "Epoch 98 [44800/60000] Loss: 0.027212 | Grad Norm: 0.999999\n",
      "Epoch 98 [51200/60000] Loss: 0.027302 | Grad Norm: 0.001111\n",
      "Epoch 98 [57600/60000] Loss: 0.028003 | Grad Norm: 0.298447\n",
      "Accuracy on test images: 92.58%\n",
      "Epoch 99 [0/60000] Loss: 0.016401 | Grad Norm: 0.670310\n",
      "Epoch 99 [6400/60000] Loss: 0.027042 | Grad Norm: 0.018800\n",
      "Epoch 99 [12800/60000] Loss: 0.026440 | Grad Norm: 0.003733\n",
      "Epoch 99 [19200/60000] Loss: 0.027302 | Grad Norm: 0.002945\n",
      "Epoch 99 [25600/60000] Loss: 0.028105 | Grad Norm: 0.999999\n",
      "Epoch 99 [32000/60000] Loss: 0.027607 | Grad Norm: 0.245420\n",
      "Epoch 99 [38400/60000] Loss: 0.027503 | Grad Norm: 0.242036\n",
      "Epoch 99 [44800/60000] Loss: 0.027235 | Grad Norm: 0.000025\n",
      "Epoch 99 [51200/60000] Loss: 0.027358 | Grad Norm: 0.092309\n",
      "Epoch 99 [57600/60000] Loss: 0.027142 | Grad Norm: 0.329356\n",
      "Accuracy on test images: 92.45%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define CNN architecture with Batch Normalization\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch Norm\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch Norm\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Batch Norm\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 128)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(128)  # Batch Norm\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(64)  # Batch Norm\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Apply Batch Norm\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Apply Batch Norm\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Apply Batch Norm\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))  # Apply Batch Norm\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn_fc2(self.fc2(x)))  # Apply Batch Norm\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Custom Bayes Loss function\n",
    "def Bayes_loss(output, target):\n",
    "    batch_size = output.size(0)\n",
    "    num_classes = output.size(1)\n",
    "    \n",
    "    # Create a mask for the losses\n",
    "    mask = torch.arange(num_classes, device=target.device).expand(batch_size, num_classes)\n",
    "    target_expanded = target.unsqueeze(1).expand_as(mask)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = (mask >= target_expanded).float() * output\n",
    "    loss += (mask == (target_expanded - 1)).float() * (1 - output)\n",
    "    \n",
    "    return loss.sum() / batch_size\n",
    "\n",
    "# Initialize the model, optimizer, and scheduler\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = FashionMNISTModel().to(device)\n",
    "\n",
    "# Use RMSprop optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler: Reduce LR when a metric has stopped improving\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Gradient clipping value\n",
    "clip_value = 1.0\n",
    "\n",
    "# Define the training function with gradient clipping and monitoring\n",
    "def train(epoch, loader, model, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate Bayes loss\n",
    "        be = Bayes_loss(output[:, 1:10], target)\n",
    "        \n",
    "        # Backpropagation with gradient clipping\n",
    "        be.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += be.item()\n",
    "        \n",
    "        # Monitor gradient norms\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        \n",
    "        # Print loss and gradient norm every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(loader.dataset)}] '\n",
    "                  f'Loss: {avg_loss:.6f} | Grad Norm: {total_norm:.6f}')\n",
    "            \n",
    "    # Scheduler step if applicable\n",
    "    if scheduler:\n",
    "        scheduler.step(running_loss / len(loader))\n",
    "\n",
    "# Evaluate the model\n",
    "def test(loader, model, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, targets in loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # Get the predicted class\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Accumulate total and correct predictions\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy on test images: {100 * accuracy:.2f}%')\n",
    "\n",
    "# Training loop with test evaluation after each epoch\n",
    "for epoch in range(1, 100):  # Increased number of epochs\n",
    "    train(epoch, trainloader, model, optimizer, scheduler)\n",
    "    test(testloader, model, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
