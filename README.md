# BOLT-loss

> **Implementation & reproducibility companion for the ICASSPâ€¯2025 paper  
> â€œUniversal Training of Neural Networks to Achieve Bayes Optimal Classification Accuracy.â€**

This repository provides a ** PyTorch reference implementation** of the **B**ayesâ€‘optimal **O**ptimal **L**earning **T**hreshold (BOLT) loss together with
Jupyter notebooks and scripts that reproduce the experiments reported in the paper.

---

## âœ¨ What is BOLT?

BOLT minimises a novel, sampleâ€‘level upper bound on the Bayes error rate.  
In practice this gives **equal or better accuracy** than crossâ€‘entropy while being **hyperâ€‘parameter free**.

```
Crossâ€‘entropy  â†’ minimises âˆ’logâ€¯p(labelâ€¯|â€¯x)
BOLT           â†’ minimises an upper bound on Bayes error
```

---

## ğŸ—‚ Repository layout

```
BOLT-loss/
â”œâ”€â”€ bolt_loss.py              â† standâ€‘alone PyTorch implementation
â”œâ”€â”€ notebooks/                â† demo & reproduction notebooks
â”‚   â”œâ”€â”€ toy_example.ipynb
â”‚   â”œâ”€â”€ MNIST.ipynb
â”‚   â”œâ”€â”€ Fashion_MNIST_BOLT_vs_CE.ipynb
â”‚   â””â”€â”€ Cifar10_BOLT_loss.ipynb
â””â”€â”€ README.md                 â† you are here
```

---

## ğŸ”§ Installation

```bash
git clone https://github.com/MohammadrezaTavasoli/BOLT-loss.git
cd BOLT-loss
conda create -n bolt python=3.10 -y     # or use venv
conda activate bolt
pip install -r requirements.txt         # torch â‰¥â€¯2.2, torchvision, numpy, â€¦
```

> **macOS / Apple Silicon** â€” code autoâ€‘detects `torch.device("mps")`.  
> GPU acceleration on NVIDIA works the usual way with CUDA.

---

## ğŸš€ Quick start (script)

```bash
python examples/train_cifar10.py --epochs 100 --loss bolt
```

*Expected accuracy:*â€¯â‰ˆâ€¯**93.3â€¯%** on CIFARâ€‘10 with ResNetâ€‘18.

---

## ğŸ’¡ BOLT loss in 15â€¯lines

```python
import torch
import torch.nn.functional as F  # for softmax

def BOLT_loss(logits: torch.Tensor,
              targets: torch.Tensor,
              norm: str = "l2") -> torch.Tensor:
    """Batchâ€‘averaged BOLT loss.

    logits : (B,â€¯K)  raw network outputs for Kâ‰¥2 classes
    targets: (B,)    groundâ€‘truth labels in 0â€¦Kâˆ’1
    norm   : "l1" or "l2" â€” absolute or squared error variant
    """
    probs = F.softmax(logits, dim=1)[:, 1:]   # drop classâ€‘0, shape (B,â€¯Kâˆ’1)
    B, C  = probs.size()

    class_mask = torch.arange(C, device=targets.device).expand(B, C)
    tgt = targets.unsqueeze(1).expand_as(class_mask)

    loss_mat  = (class_mask >= tgt).float() * probs
    loss_mat += (class_mask == (tgt - 1)).float() * (1.0 - probs)

    if norm.lower() == "l2":
        return loss_mat.pow(2).sum() / B
    if norm.lower() == "l1":
        return loss_mat.abs().sum() / B
    raise ValueError("norm must be 'l1' or 'l2'")
```

---

## ğŸ“Š Reproducing the paper

Open any notebook in `notebooks/` and run it endâ€‘toâ€‘end; results, plots and
LaTeXâ€‘ready tables will be generated automatically.

| Dataset   | Model      | BOLT | Crossâ€‘entropy |
|-----------|------------|------|---------------|
| CIFARâ€‘10  | ResNetâ€‘18  | **93.29â€¯%** | 91.95â€¯% |
| IMDb      | BERTâ€‘base  | **94.56â€¯%** | 93.51â€¯% |
| MNIST     | 4â€‘layer CNN | 99.29â€¯% | 99.29â€¯% |

---

## ğŸ“ Citation

```bibtex
@inproceedings{tavasoli2025bolt,
  title     = {Universal Training of Neural Networks to Achieve Bayes Optimal Classification Accuracy},
  author    = {Mohammadreza Tavasoli Naeini and Ali Bereyhi and Morteza Noshad and Ben Liang and Alfred O. Hero III},
  booktitle = {Proc. ICASSP},
  year      = {2025}
}
```

---

## ğŸ“„ License

Released under the MIT license â€“ see `LICENSE` for details.

---

Happy training! ğŸš€
